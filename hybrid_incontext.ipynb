{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'powell_river',\n",
       " 'location type': 'municipality',\n",
       " 'meeting type': 'regular_council',\n",
       " 'data type': 'minutes',\n",
       " 'meeting date': Timestamp('2024-03-21 00:00:00'),\n",
       " 'transcript': 'Yes',\n",
       " 'comment': nan}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one file:\n",
    "pdf_folder = 'data/batch/'\n",
    "metting_name = \"cnv_mcp_rgc_min__2024-03-11__01\"\n",
    "metting_name = \"gib_mcp_rgc_min__2024-04-09__01\"\n",
    "metting_name = \"qtt_reg_rgc_min__2024-04-03__01\"\n",
    "metting_name = \"dnv_mcp_rgc_min__2024-02-05__01\"\n",
    "metting_name = \"bwi_mcp_rgc_min__2024-03-25__01\"\n",
    "metting_name = \"ssc_reg_rgc_min__2024-03-28__01\"\n",
    "metting_name = \"van_mcp_rgc_min__2024-04-09__01\"\n",
    "metting_name = \"qtt_reg_rgc_min__2024-04-03__01\"\n",
    "\n",
    "metting_name = \"rmd_mcp_rgc_min__2024-03-25__01\"\n",
    "metting_name = \"cnv_mcp_rgc_min__2024-04-15__01\"\n",
    "metting_name = \"mtv_reg_rgc_min__2024-02-23__01\"\n",
    "metting_name = \"sqm_mcp_cow_min__2024-03-19__01\"\n",
    "\n",
    "\n",
    "\n",
    "metting_name = \"pwr_mcp_rgc_min__2024-03-21__01\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pdf_file = metting_name + \".pdf\"\n",
    "pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "# read the info from a xlsx file\n",
    "df = pd.read_excel(\"data/meetingmap.xlsx\")\n",
    "df = df.set_index('standard name')\n",
    "# get the info of the meeting\n",
    "meeting_info = df.loc[pdf_file]\n",
    "# return all the columns for the meeting\n",
    "meeting_info = meeting_info.to_dict()\n",
    "meeting_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# create a dictionary to store the text of each pdf and the metadata and if there is a txt with the same name\n",
    "context = {}\n",
    "    \n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "context['pdf'] = text\n",
    "# get the metadata\n",
    "metadata = fitz.open(pdf_path).metadata\n",
    "context['metadata'] = metadata\n",
    "# check if there is a txt with the same name\n",
    "txt_path = os.path.join(pdf_folder, pdf_file.replace('.pdf', '.txt'))\n",
    "if os.path.exists(txt_path):\n",
    "    with open(txt_path, 'r') as file:\n",
    "        context['transcript'] = file.read()\n",
    "\n",
    "else:\n",
    "        context['transcript'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pdf', 'metadata', 'transcript'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of themes:  56\n"
     ]
    }
   ],
   "source": [
    "# load the theme keywords from themekeywordmap.xlsx\n",
    "# convert it to a dictionary with the theme as key and different keywords as a list\n",
    "# it has two columns: theme and single phrase\n",
    "# combine all the phrases that have the same theme in  a list as follows\n",
    "# category_keywords = {\n",
    "#     \"theme 1\": [\"phrase 1\", \"phrase 2\", \"phrase 3\"],\n",
    "#     \"theme 2\": [\"phrase 4\", \"phrase 5\", \"phrase 6\", \"phrase 7\"]\n",
    "# }\n",
    "df = pd.read_excel(\"data/themekeywordmap.xlsx\")\n",
    "category_keywords = {}\n",
    "for theme, group in df.groupby('theme'):\n",
    "    category_keywords[theme] = group['phrase'].tolist()\n",
    "print(\"total number of themes: \", len(category_keywords.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Segment using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_patterns = [\n",
    "    r\"MOVED by\",\n",
    "    r\"SECONDED by\",\n",
    "    r\"WHEREAS\",\n",
    "    r\"THEREFORE BE IT RESOLVED THAT\",\n",
    "    r\"CARRIED UNANIMOUSLY\",\n",
    "    r\"REJECTED\",\n",
    "    r\"THAT\",\n",
    "    r\"APPROVED\",\n",
    "    r\"ADOPTED\",\n",
    "    r\"RESOLVED\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_document(document, patterns):\n",
    "    \"\"\"\n",
    "    Segment the document based on defined patterns.\n",
    "    \"\"\"\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "    segments = re.split(combined_pattern, document, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Filter out empty segments and strip whitespace\n",
    "    segments = [seg.strip() for seg in segments if seg.strip()]\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keywords(segment, category_keywords):\n",
    "    \"\"\"\n",
    "    Match segments against category keywords.\n",
    "    \"\"\"\n",
    "    matched_categories = []\n",
    "    for category, keywords in category_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', segment, re.IGNORECASE):\n",
    "                matched_categories.append(category)\n",
    "                break  # Break after the first match to avoid redundant checks\n",
    "    return matched_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segments(segments, category_keywords):\n",
    "    \"\"\"\n",
    "    Combine nearby segments and filter them based on category keywords.\n",
    "    \"\"\"\n",
    "    combined_segments = []\n",
    "    current_segment = \"\"\n",
    "    current_categories = set()\n",
    "\n",
    "    for segment in segments:\n",
    "        matched_categories = match_keywords(segment, category_keywords)\n",
    "        if matched_categories:\n",
    "            if current_segment:\n",
    "                combined_segments.append({\n",
    "                    \"text\": current_segment,\n",
    "                    \"categories\": list(current_categories)\n",
    "                })\n",
    "                current_segment = segment\n",
    "                current_categories = set(matched_categories)\n",
    "            else:\n",
    "                current_segment = segment\n",
    "                current_categories.update(matched_categories)\n",
    "        else:\n",
    "            current_segment += \" \" + segment\n",
    "    \n",
    "    if current_segment:\n",
    "        combined_segments.append({\n",
    "            \"text\": current_segment,\n",
    "            \"categories\": list(current_categories)\n",
    "        })\n",
    "\n",
    "    return combined_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = segment_document(context['pdf'], split_patterns)\n",
    "combined_segments = combine_segments(segments, category_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total initial segments: 4\n"
     ]
    }
   ],
   "source": [
    "# initial assesment results\n",
    "print(\"Total initial segments:\", len(combined_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_nlp = json.dumps(combined_segments, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segments_v2(segments, category_keywords):\n",
    "    \"\"\"\n",
    "    Combine nearby segments and filter them based on category keywords.\n",
    "    \"\"\"\n",
    "    combined_segments = []\n",
    "    current_segment = \"\"\n",
    "    current_categories = set()\n",
    "    last_matched_categories = set()\n",
    "\n",
    "    for segment in segments:\n",
    "        matched_categories = match_keywords(segment, category_keywords)\n",
    "        if matched_categories:\n",
    "            # If current segment is not empty and the new segment has different categories,\n",
    "            # add the current segment to combined_segments and start a new one\n",
    "            if current_segment and matched_categories != last_matched_categories:\n",
    "                combined_segments.append({\n",
    "                    \"text\": current_segment,\n",
    "                    \"categories\": list(current_categories)\n",
    "                })\n",
    "                current_segment = segment\n",
    "                current_categories = set(matched_categories)\n",
    "            else:\n",
    "                current_segment += \" \" + segment\n",
    "                current_categories.update(matched_categories)\n",
    "            last_matched_categories = matched_categories\n",
    "        else:\n",
    "            current_segment += \" \" + segment\n",
    "    \n",
    "    # Append the last segment\n",
    "    if current_segment:\n",
    "        combined_segments.append({\n",
    "            \"text\": current_segment,\n",
    "            \"categories\": list(current_categories)\n",
    "        })\n",
    "\n",
    "    return combined_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proposal_indicators = [\"Moved\", \"Seconded\", \"Motion\", \"Carried\", \"Proposal\", \"Passed\", \"Adopted\", \"Adoption\", \"Rejected\", \"Lost\", \"Moved\", \"approve\", \"Seconded\" , \"Adopt\", \"Resolution\", \"rejected\", \"Ordinance\", \"defeated\", \"discussed\", \"withdrawn\", \"tabled\", \"Amendment\", \"Amendment\", \"Recommendation\", \"granted\", \"Petition\", \"denied\", \"Vote\", \"result\"]\n",
    "# lower case it\n",
    "Proposal_indicators = [x.lower() for x in Proposal_indicators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_segments_v3(segments, category_keywords, Proposal_indicators):\n",
    "    combined_proposals = []\n",
    "    temp_segment = \"\"\n",
    "\n",
    "    def contains_proposal_indicators(text):\n",
    "            return any(indicator in text.lower() for indicator in Proposal_indicators)\n",
    "    def match_keywords(segment, category_keywords):\n",
    "        \"\"\"\n",
    "        Match segments against category keywords.\n",
    "        \"\"\"\n",
    "        matched_categories = []\n",
    "        for category, keywords in category_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if re.search(r'\\b' + re.escape(keyword) + r'\\b', segment, re.IGNORECASE):\n",
    "                    matched_categories.append(category)\n",
    "        return matched_categories\n",
    "  \n",
    "    \n",
    "    for seg in segments:\n",
    "        \"\"\"Splits/combine the segments such that it contains one proposals indicator.\"\"\"\n",
    "        for line in seg.split('\\n'):\n",
    "            temp_segment += line + \" \"\n",
    "            if contains_proposal_indicators(line):\n",
    "                categories = match_keywords(temp_segment, category_keywords)\n",
    "                combined_proposals.append({\n",
    "                    \"text\": temp_segment.strip(),\n",
    "                    \"categories\": list(set(categories))\n",
    "                })\n",
    "                temp_segment = \"\"\n",
    "                category_set = set()\n",
    "    if temp_segment.strip():\n",
    "        # append it to the last proposal\n",
    "        categories = match_keywords(temp_segment, category_keywords)\n",
    "        combined_proposals[-1] = {\n",
    "                    \"text\": combined_proposals[-1][\"text\"] + \" \" + temp_segment.strip(),\n",
    "                    \"categories\": list(set(categories+ combined_proposals[-1][\"categories\"])),\n",
    "                }\n",
    "    for i in reversed(range(len(combined_proposals))):\n",
    "        if len(combined_proposals[i][\"text\"]) < 50:\n",
    "            combined_proposals[i-1][\"text\"] = combined_proposals[i-1][\"text\"] + \" \" + combined_proposals[i][\"text\"]\n",
    "            # remove the current segment\n",
    "            del combined_proposals[i]\n",
    "    return combined_proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_segments = combine_segments(segments, category_keywords)\n",
    "#combined_segments = combine_segments_v2(segments, category_keywords)\n",
    "combined_segments = combine_segments_v3(segments, category_keywords, Proposal_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"City of Powell River Revenue Anticipation  Borrowing Bylaw 2740,2024\" be read a first, second, and third time.  CARRIED',\n",
       " 'categories': []}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_segments[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: find category based on vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Generate embeddings for each category based on keywords\n",
    "category_embeddings = {}\n",
    "for category, keywords in category_keywords.items():\n",
    "    category_embeddings[category] = model.encode(keywords, convert_to_tensor=True)\n",
    "\n",
    "# Flatten category embeddings for FAISS indexing\n",
    "flat_embeddings = []\n",
    "category_indices = []\n",
    "for category, embeddings in category_embeddings.items():\n",
    "    for embedding in embeddings:\n",
    "        flat_embeddings.append(embedding.cpu().detach().numpy())\n",
    "        category_indices.append(category)\n",
    "\n",
    "flat_embeddings = np.vstack(flat_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = flat_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(flat_embeddings)\n",
    "\n",
    "# Save FAISS index and data for later use\n",
    "faiss.write_index(index, 'data/faiss_index.bin')\n",
    "np.save('data/category_indices.npy', category_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elbow_point_indices(data):\n",
    "    # Sort the data and keep track of the original indices\n",
    "    sorted_data_with_indices = sorted((val, idx) for idx, val in enumerate(data))\n",
    "    sorted_data = [val for val, idx in sorted_data_with_indices]\n",
    "    sorted_indices = [idx for val, idx in sorted_data_with_indices]\n",
    "    \n",
    "    # Calculate the differences between consecutive elements\n",
    "    differences = np.diff(sorted_data)\n",
    "    \n",
    "    # Find the index where the difference significantly increases\n",
    "    elbow_index = np.argmax(differences)\n",
    "    \n",
    "    # Find the elbow point value\n",
    "    elbow_point = sorted_data[elbow_index]\n",
    "    \n",
    "    # Find indices of elements to remove (smaller than or equal to the elbow point)\n",
    "    to_remove_indices = [idx for idx, val in enumerate(data) if val <= elbow_point]\n",
    "    return to_remove_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_database(text, model, index, category_indices,num_categories_to_search=20):\n",
    "    \"\"\"\n",
    "    Query the FAISS index with a text embedding and return the most relevant categories.\n",
    "    Avoid returning repetitive categories and apply a similarity threshold.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the text\n",
    "    text_embedding = model.encode([text], convert_to_tensor=True)\n",
    "    text_embedding = text_embedding.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    # Search the index for the most similar embeddings\n",
    "    distances, indices = index.search(text_embedding, num_categories_to_search)\n",
    "    to_remove = find_elbow_point_indices(distances[0])\n",
    "\n",
    "    # Filter out categories based on the threshold and avoid repetitions\n",
    "    seen_categories = set()\n",
    "    categories_by_vd = []\n",
    "    for idx in  indices[0]:\n",
    "        if idx in to_remove:\n",
    "            continue\n",
    "        category = category_indices[idx]\n",
    "        if category not in seen_categories:\n",
    "            categories_by_vd.append(category)\n",
    "            seen_categories.add(category)\n",
    "\n",
    "    \n",
    "\n",
    "    return categories_by_vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"FEBRUARY 27, 2024  Special Council Meeting Minutes  File 0550-03-0002      CITY OF POWELL RIVER    Minutes of the Special Council Meeting held in the Council Chambers, City Hall on Tuesday,  February 27, 2024 at 3:30 PM.    PRESENT:  Mayor R.J. Woznow  Councillor E.L. Almeida  Councillor G.W.F. Doubt  Councillor C.A. Elliott  Councillor T.E. Isakson  Councillor J.G. Palm  Councillor R.R.D. Southcott          ALSO PRESENT:  Lisa Bhopalsingh, Chief Administrative Officer  Jessica Lefort, Deputy Corporate Officer/Recording  Secretary  Mallory Denniston, Chief Financial Officer  Jason Gow, Director of Planning Services  Jamie Bretzlaff, Director of Parks, Recreation and  Culture  Rod Fraser, Manager of Operational Services  Martin Drakeley, Director of Fire and Emergency  Services  Karsten Sian, Members of the Public  Media Representatives        1  ADOPTION OF AGENDA 1.1  Res 24-115  Moved and seconded\",\n",
      "  \"categories\": [\n",
      "    \"physical_activity\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"other__emergency_management\",\n",
      "    \"housing__permits\",\n",
      "    \"youth_children\",\n",
      "    \"mental_health\",\n",
      "    \"housing\",\n",
      "    \"healthy_built_environments__natural_environments/green_infrastructure\",\n",
      "    \"injury_prevention__youth_self-harm\"\n",
      "  ],\n",
      "  \"id\": 0\n",
      "}\n",
      "{\n",
      "  \"text\": \"the agenda for the February 27, 2024 Special  Council meeting be .  CARRIED\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"housing__permits\",\n",
      "    \"injury_prevention__youth_self-harm\"\n",
      "  ],\n",
      "  \"id\": 1\n",
      "}\n",
      "{\n",
      "  \"text\": \"2  BYLAWS      2.1  Draft Bylaw 2740, 2024 (Revenue Anticipation Borrowing)  A bylaw to provide for the borrowing of money in anticipation of revenue      Res 24-116  Moved and seconded\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"poverty_affordability\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 2\n",
      "}\n",
      "{\n",
      "  \"text\": \"\\\"City of Powell River Revenue Anticipation  Borrowing Bylaw 2740,2024\\\" be read a first, second, and third time.  CARRIED\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"climate_change\",\n",
      "    \"harm_reduction\",\n",
      "    \"environmental_exposures__liquid_waste/_wastewater/sewage\"\n",
      "  ],\n",
      "  \"id\": 3\n",
      "}\n",
      "{\n",
      "  \"text\": \"2.2  Draft Bylaw 2741 (2023 Financial Plan Amendment)\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"poverty_affordability\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 4\n",
      "}\n",
      "{\n",
      "  \"text\": \"A bylaw to amend City of Powell River Five-Year Financial Plan Bylaw 2715,  2023        FEBRUARY 27, 2024  Special Council Meeting Minutes  File 0550-03-0002      Res 24-117  Moved and seconded\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"healthy_built_environments__community_design/planning\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 5\n",
      "}\n",
      "{\n",
      "  \"text\": \"the 2023 \\\"Five-Year Financial Plan Bylaw 2715,  2023 Amendment Bylaw 2741\\\", 2024 be read a first, second, and third time. CARRIED\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"poverty_affordability\",\n",
      "    \"housing\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 6\n",
      "}\n",
      "{\n",
      "  \"text\": \"2.3  Draft 2024 Five- Year Financial Plan Bylaw 2742, 2024      Res 24-118  Moved and seconded\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"indigenous_relations__decolonization\"\n",
      "  ],\n",
      "  \"id\": 7\n",
      "}\n",
      "{\n",
      "  \"text\": \"Draft 2 of the 2024 Five-Year Financial Plan be  amended to remove all funding for the Zunga bus.     Opposed: Councillors Almeida, Isakson, Doubt, Southcott and Elliott  DEFEATED Res 24-119  Moved and seconded\",\n",
      "  \"categories\": [\n",
      "    \"healthy_built_environments__active_transportation\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"mental_health\",\n",
      "    \"other__emergency_management\",\n",
      "    \"healthy_built_environments__community_design/planning\"\n",
      "  ],\n",
      "  \"id\": 8\n",
      "}\n",
      "{\n",
      "  \"text\": \"Draft \\\"City of Powell River Five-Year Financial Plan  Bylaw 2742, 2024\\\" be read a first time.  Opposed: Councillor Palm  CARRIED\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"healthy_built_environments__community_design/planning\"\n",
      "  ],\n",
      "  \"id\": 9\n",
      "}\n",
      "{\n",
      "  \"text\": \"3  UNFINISHED BUSINESS      3.1  Select Committee on City Owned Properties - Follow Up Discussion    Members of Council discussed direction previously provided to the Committee.\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 10\n",
      "}\n",
      "{\n",
      "  \"text\": \"As Council\\u2019s resolution from December 7, 2023 will be on the next Committee\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"indigenous_relations__reconciliation\",\n",
      "    \"other__emergency_management\",\n",
      "    \"housing__permits\",\n",
      "    \"mental_health\",\n",
      "    \"housing\",\n",
      "    \"injury_prevention__youth_self-harm\"\n",
      "  ],\n",
      "  \"id\": 11\n",
      "}\n",
      "{\n",
      "  \"text\": \"agenda, no further decisions were made at this time.       4  NEW BUSINESS      4.1  Public Comments attributed to Maynard Harry    Mayor Woznow stated Council does not support hate speech and will not  speak about any individual directly.      5  QUESTIONS      5.1  Prior to adjournment, the Chair received questions regarding the following  agenda items:   \\uf0b7  Item 2.3 \\u2013 Draft Bylaw 2742, 2024 (2024 Financial Plan)  \\uf0b7  Item 3.1 \\u2013 Select Committee on City Owned Properties \\u2013 Follow Up  Discussion     The Chair ensured enough time had passed to allow members of the\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"housing__permits\",\n",
      "    \"injury_prevention__youth_self-harm\"\n",
      "  ],\n",
      "  \"id\": 12\n",
      "}\n",
      "{\n",
      "  \"text\": \"public watching remotely to call in.     Staff responded no calls or emails with questions had been received.        FEBRUARY 27, 2024  Special Council Meeting Minutes  File 0550-03-0002      6  ADJOURNMENT      6.1  Res 24-120  Moved and seconded\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"other__emergency_management\",\n",
      "    \"healthy_built_environments\",\n",
      "    \"youth_children\",\n",
      "    \"mental_health\",\n",
      "    \"communicable_disease\",\n",
      "    \"housing\",\n",
      "    \"equity_and_diversity__accessibility\",\n",
      "    \"injury_prevention__youth_self-harm\"\n",
      "  ],\n",
      "  \"id\": 13\n",
      "}\n",
      "{\n",
      "  \"text\": \"the meeting adjourn at 5:02 pm.  CARRIED CERTIFIED CORRECT  Jessica Lefort  Deputy Corporate Officer\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"housing__permits\",\n",
      "    \"youth_children\",\n",
      "    \"injury_prevention__youth_self-harm\"\n",
      "  ],\n",
      "  \"id\": 14\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load the FAISS index and category indices\n",
    "index = faiss.read_index('data/faiss_index.bin')\n",
    "category_indices = np.load('data/category_indices.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# Verify categories and update segments\n",
    "for segment in combined_segments:\n",
    "    suggested_categories = query_vector_database(segment['text'], model, index, category_indices)\n",
    "    # making the categories unique\n",
    "    suggested_categories = list(set(suggested_categories))\n",
    "    segment[\"categories_by_vd\"] = suggested_categories\n",
    "    segment[\"id\"] = combined_segments.index(segment)\n",
    "\n",
    "# Print the updated categorized segments\n",
    "for segment in combined_segments:\n",
    "    print(json.dumps(segment, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the agenda for the February 27, 2024 Special  Council meeting be .  CARRIED',\n",
       " 'categories': [],\n",
       " 'categories_by_vd': ['housing',\n",
       "  'housing__permits',\n",
       "  'injury_prevention__youth_self-harm'],\n",
       " 'id': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_segments[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3: lang-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LANGCHAIN_API_KEY from the environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the context['pdf'] and create vector store\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "def filter_none_values(metadata):\n",
    "    return {k: v for k, v in metadata.items() if v is not None}\n",
    "filtered_metadata = filter_none_values(context['metadata'])\n",
    "\n",
    "text_splits = text_splitter.split_text(context['pdf'])\n",
    "metadata_list = [filtered_metadata] * len(text_splits)\n",
    "\n",
    "# Add  text_splits to vectorDB with  nomic-embed-text-v1.5  and inference_mode=\"local\n",
    "vectorstore = Chroma.from_texts(\n",
    "        texts=text_splits,\n",
    "        metadatas=metadata_list,\n",
    "        # embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "        embedding=embeddings.OllamaEmbeddings(model=\"nomic-embed-text:v1.5\"),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_count = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are an assitance that detect the number of unique proposals with a specific categoty (theme) in a suggested proposal of a council meeting note. \\n\n",
    "You are provided with suggested proposal and its category as user prompt. \\n\n",
    "The goal is to find count of unique proposal in the suggested proposal with the specific category. \\n\n",
    "Give a integer count of unique proposal as a JSON with single key 'count'. \\n\n",
    "\n",
    "example:\n",
    "proposal: ### Minutes of the Council meeting of February 6, 2024, be approved.\\n CARRIED UNANIMOUSLY \\n Council Meeting\\n Minutes, February 27, 2024 3\\n 3. Council (City Finance and Services) \\n MOVED by Councillor Dominato\\n SECONDED by Councillor Carr\\n THAT the Minutes of the Council meeting following the Standing Committee on City\\n Finance and Services meeting of February 7, 2024, be approved.\\n CARRIED UNANIMOUSLY\\n 4. Court of Revision (Business Improvement Areas) - February 8, 2024\\n MOVED by Councillor Bligh\\n SECONDED by Councillor Zhou\\n  THAT the Minutes of the Court of Revision (Business Improvement Areas) meeting of\\n February 8, 2024, be approved.\\n CARRIED UNANIMOUSLY\\n MATTERS ADOPTED ON CONSENT \n",
    "categories: [\"City Finance and Services\", \"Business Improvement Areas\"]\n",
    "output: \"count\": 2\n",
    "\n",
    "Let's think step by step. Here are the steps to solve the task:\n",
    "1. Validity check: A proposal should suggest some action or decision and it should have a unique decision. \n",
    "2. Proposal Count: See if the suggested proposal includes more than one proposal. calculate the count.\n",
    "5. Write: Only return the count as integer in a JSON format with a single key 'count'.\n",
    "\n",
    "RULES:\n",
    "- your output MUST HAVE the exact JSON format.\n",
    "- Your answer must not include any speculation or inference. Do not assume or change dates and times. Only provide information that is explicitly stated in the context.\n",
    "- An answer is considered grounded if **all** information in **every** sentence in the answer is **explicitly** mentioned in the source context, **no** extra information is added and **no** inferred information is added.\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Category:\\n {category} \\n\\n\n",
    "    suggested_proposal:\\n {suggested_proposal} \\n\\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"category\", \"suggested_proposal\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 1}\n"
     ]
    }
   ],
   "source": [
    "num = 1\n",
    "suggested_proposal = combined_segments[num][\"text\"]\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "retrieval_counter = prompt_count | llm | JsonOutputParser()\n",
    "category = combined_segments[num][\"categories_by_vd\"]\n",
    "print(retrieval_counter.invoke({\"category\": category, \"suggested_proposal\": suggested_proposal}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def find_closest_match(original_string, substring):\n",
    "    # Initialize variables\n",
    "    closest_match_index = -1\n",
    "    highest_similarity = 0\n",
    "    substring_length = len(substring)\n",
    "    \n",
    "    # Define a function to calculate similarity ratio\n",
    "    def similarity(s1, s2):\n",
    "        return difflib.SequenceMatcher(None, s1, s2).ratio()\n",
    "    \n",
    "    # Compare the substring against all possible substrings of the same length in the original string\n",
    "    for i in range(len(original_string) - substring_length + 1):\n",
    "        current_substring = original_string[i:i + substring_length]\n",
    "        current_similarity = similarity(current_substring, substring)\n",
    "        if current_similarity > highest_similarity:\n",
    "            highest_similarity = current_similarity\n",
    "            closest_match_index = i\n",
    "    \n",
    "    return closest_match_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### Example:\n",
    "#### User Input:  \n",
    "# \"context\": \"The meeting was called to order at 10:00 AM. Moved by John Doe, the proposal to increase the budget for the community park was discussed. The committee deliberated on various aspects. Motion Carried. Moved by Tom. The agenda item to increase wages. Motion Carried. The next item\"\\n\n",
    "\"suggested_proposal\": \"increase the budget for the community park was discussed.\" \\n\\n\n",
    "#### Model correct Output:\n",
    "\"start\": \"Moved by John Doe, the proposal to increase\",\n",
    "\"end\": \" deliberated on various aspects. Motion Carried.\"\"\"\n",
    "\n",
    "prompt_boundry = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are an AI assistance to identify the adjusted boundary of a suggested_proposal within context (meeting notes). \\n\n",
    "You are given the context and the suggested proposal (a chunk of the context that is not accurately bounded). \\n\n",
    "Your task is to first locate the exact chunk (suggested_proposal) in the context, then add or remove words from it such that the new chunk include all related information about the proposal. \n",
    "Do NOT write about another proposal, just the suggested_proposal. \\n\n",
    "\n",
    "### Instructions:\n",
    "1. **Output Format**:\n",
    "    - Provide the output in JSON format with the keys 'start' and 'end'.\n",
    "    - Both 'start' and 'end' values should include atleast five and atmost ten words from the context. \n",
    "\n",
    "2. **Rules**:\n",
    "    - **Use the suggested proposal as the basis**: Ensure the adjusted boundary in the context indeed is same as the original suggested_proposal.\n",
    "    - Do not use speculations or inferences. Only provide information that is explicitly stated in the context and is about the suggested_proposal.\n",
    "    - Maintain the coherence and logical flow of the proposal.\n",
    "    - All necessary information such as the mover, the proposal title, the vote result, or note are considered related information and should be inside the boundary.\n",
    "    - ** word count**: DO NOT return less than 5 words or more than 10 words for each boundary marker 'start' and 'end'.\n",
    "    - only one proposal that is related to the suggested_proposal should be selected.\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "     Given the context and suggested proposal, follow the above instructions to determine the precise boundary. Ensure your output is in JSON with two keys 'start' and 'end'. Return ONLY and only one proposal. DO NOT write  anything other than a JSON with keys \"start\" and \"end\".\\n\\n\n",
    "    \"context\":\\n {document} \\n\\n\n",
    "    \"suggested_proposal\":\\n {suggested_proposal} \\n\\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"document\", \"suggested_proposal\"],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 'Moved and seconded that the agenda for the February 27, 2024 Special Council meeting be adopted. CARRIED', 'end': 'CARRIED'}\n"
     ]
    }
   ],
   "source": [
    "num = 1\n",
    "suggested_proposal = combined_segments[num][\"text\"]\n",
    "\n",
    "llm_boundry = ChatOllama(model=local_llm, temperature=0, keep_alive=0)\n",
    "retrieval_boundary = prompt_boundry | llm_boundry | JsonOutputParser()\n",
    "txt_chunk = retriever.invoke(suggested_proposal)[:3]\n",
    "txt_chunk = [d.page_content for d in txt_chunk]\n",
    "boundry = retrieval_boundary.invoke({\"document\": txt_chunk, \"suggested_proposal\": suggested_proposal})\n",
    "print(boundry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the agenda for the February 27, 2024 Special  Council meeting be .  CARRIED'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Moved and seconded that the agenda for the February 27, 2024 Special  Council meeting be adopted.  CARRIED '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjusted_proposal_boundary(txt_chunk, proposal, boundry):\n",
    "    start = boundry[\"start\"]\n",
    "    end = boundry[\"end\"]\n",
    "    # # check if the length of end or start is less 30\n",
    "    # if len(start) < 30:\n",
    "    #     start = start + ' ' + end[:20] + ' ' + proposal[:20]\n",
    "    # elif len(end) < 30:\n",
    "    #     end =end + ' ' +  proposal[-30:] + ' ' +  start[-30:] \n",
    "    txt = ''\n",
    "    for txts in txt_chunk:\n",
    "        txt += ' ' + txts.replace('\\n', ' ').strip()\n",
    "    start_index = find_closest_match(txt, start)\n",
    "    end_index = find_closest_match(txt[start_index+len(start)-20:], end) + start_index + len(start)-20\n",
    "    # change the end_index to the closest . or ; or end of the sentence\n",
    "    end_index1 = txt.find(' ', end_index+ len(end))\n",
    "    end_index2 = txt.find('.', end_index+ len(end))\n",
    "    # find smaller positive index and set as end_index\n",
    "    if end_index1 >= 0 and end_index2 >= 0:\n",
    "        end_index = min(end_index1, end_index2)\n",
    "    elif end_index1 >=0:\n",
    "        end_index = end_index1\n",
    "    elif end_index2 >=0:\n",
    "        end_index = end_index2\n",
    "    return txt[start_index:end_index+1]\n",
    "# get the sentence\n",
    "proposal_adj = adjusted_proposal_boundary(txt_chunk, suggested_proposal, boundry)\n",
    "\n",
    "proposal_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get suggested proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_getprop = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are an AI assistance to all the unique proposals from a all_proposals. \\n\n",
    "You are given the all_proposals: string  and  count:int from the user that shows how many proposals are in all_proposals.\\n\n",
    "Your task is to find different proposals from all_proposals such that each include all required information including proposer, title, results and etc. \n",
    "Output a JSON with keys as int and value as each individual proposal. \\n\n",
    "\n",
    "**Output Format**:\n",
    "    '1': <str, content of proposal 1>,\n",
    "    '2': <str, content of proposal 2>,\n",
    "    ...\n",
    "    'count': <str, content of proposal count>,\n",
    "\n",
    "### Instructions:\n",
    "1. **approach**: \n",
    "    - Find count number of unique proposals from all_proposals.\n",
    "    - Provide the output as a JSON with len equal to count and write unique proposals as values .\n",
    "\n",
    "2. **Rules**:\n",
    "    - **Use the all_proposals as the basis**: Ensure your individual proposals are indeed from  the original all_proposals.\n",
    "    - Do not use speculations or inferences. Only provide information that is explicitly stated in the all_proposals.\n",
    "    - Maintain the coherence and logical flow of the proposal.\n",
    "    - All necessary information such as the mover, the proposal title, the vote result, or note are considered related information and should be included for one proposal.\n",
    "    - ** word count**: DO NOT return less than 5 words or more than 20 words for proposals.\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "     Given the all_proposals and count, follow the above instructions to determine the individual proposals. Ensure your output is only JSON. size of JSON should be equal to count. Do NOT return anythings else other than the JSON. \\n\\n\n",
    "    \"all_proposals\":\\n {all_proposals} \\n\\n\n",
    "    \"count\":\\n {count} \\n\\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"all_proposals\", \"count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'the agenda for the February 27, 2024 Special Council meeting be CARRIED BYLAWS 2.1 Draft Bylaw 2740, 2024 (Revenue Anticipation Borrowing) A bylaw to provide for the borrowing of money in anticipation of revenue Res 24-116 Moved and seconded', '2': 'the agenda for the February 27, 2024 Special Council meeting be CARRIED BYLAWS 2.1 Draft Bylaw 2740, 2024 (Revenue Anticipation Borrowing) A bylaw to provide for the borrowing of money in anticipation of revenue Res 24-116 Moved and seconded'}\n"
     ]
    }
   ],
   "source": [
    "num = 1\n",
    "all_proposals = combined_segments[num][\"text\"] + ' ' + combined_segments[num+1][\"text\"]\n",
    "\n",
    "llm_getprop = ChatOllama(model=local_llm, temperature=0, keep_alive=0)\n",
    "retrieval_prop = prompt_getprop | llm_getprop | JsonOutputParser()\n",
    "prop_dict = retrieval_prop.invoke({\"all_proposals\": all_proposals, \"count\": 2})\n",
    "print(prop_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feaure extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "\n",
    "# Schema for structured response\n",
    "class ProposalFeatures(BaseModel):\n",
    "    title: str = Field(description=\"A short title of the proposal\", required=True)\n",
    "    category_llm: List[str] = Field(description=f\"category if the proposal choosen from suggested category by user\", required=True)\n",
    "    vote_result: str = Field(description=\"The result of the vote\", required=True)\n",
    "    future_date: Optional[str] = Field(description=\"The future date of the proposal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt_extraction =PromptTemplate(\n",
    "    template= \"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are an AI assistance that extract related information about a suggested_proposal from the context (meeting notes). \\n\n",
    "\n",
    "Instructuctions:\n",
    "The output should be in json format and structured with the following schema:\n",
    "\n",
    "class ProposalFeatures(BaseModel):\n",
    "    title: str = Field(description=\"A short title of the proposal\", required=True)\n",
    "    category_llm: List[str] = Field(description=f\"proposal category from Suggested_categories\", required=True)\n",
    "    vote_result: str = Field(description=\"The result of the vote\", required=True)\n",
    "    future_date: Optional[str] = Field(description=\"future date of the proposal only if exist\")\n",
    "    \n",
    "RULES: \n",
    "1. Do not use speculations or inferences. All information should be only related to the Suggested_proposal from the context by the user. \n",
    "2. The category_llm should be choosed from the Suggested_categories by the user. It can be all or some of it that is related to the proposal. \n",
    "3. If there was multiple future dates, seperate by comma as a string. DO NOT return a list.\n",
    "4. If you did not find any future date, DO NOT return future_date key in the output.\n",
    "5. DO NOT return null values for any key.\n",
    "\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    \"context\":\\n {document} \\n\\n\n",
    "    \"suggested_proposal\":\\n {suggested_proposal} \\n\\n\n",
    "    \"Suggested_categories\":\\n {category} \\n\\n\n",
    "     Given the context, return title (string), category_llm (list of string), vote_result (string), and future_date (optional string) of suggested_proposal in context  with a JSON formart as mentioned in the above instruction and rules. DO NOT return any other key. Output should be of class ProposalFeatures.  \\n\\n\n",
    "     <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"document\", \"suggested_proposal\", \"category\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProposalFeatures(title='the agenda for the February 27, 2024 Special Council meeting be adopted. CARRIED', category_llm=['housing'], vote_result='CARRIED', future_date='February 27, 2024')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 1\n",
    "suggested_proposal = combined_segments[num][\"text\"]\n",
    "category = combined_segments[num][\"categories_by_vd\"]\n",
    "txt_chunk = retriever.invoke(suggested_proposal)[:1]\n",
    "txt_chunk = [d.page_content for d in txt_chunk] \n",
    "llm_extraction  = OllamaFunctions(model=local_llm, format=\"json\", temperature=0, keep_alive=0)\n",
    "\n",
    "# Chain\n",
    "structured_llm = llm_extraction.with_structured_output(ProposalFeatures)\n",
    "chain_feature = prompt_extraction | structured_llm\n",
    "\n",
    "result = chain_feature.invoke({'document':txt_chunk,'suggested_proposal': suggested_proposal, 'category': category})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langGraph control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Regional Board  Minutes     April 3, 2024, 3:30 PM  Regional District Boardroom         Present:  Chair Clayton Brander  Director Jason Lennox  Director Mark Gisborne  Director Sandy McCormick  Director Andrew Fall  Director Cindy Elliott  Alternate Director Robert Southcott    Regrets:   Director George Doubt     Staff Present:  Al Radke, Chief Administrative Officer  Michelle Jones, Manager of Administrative Services  Linda Greenan, Manager of Financial Services  Arnold Schwabe, Manager of Asset Management and Strategic Initiatives  Patrick Devereaux, Manager of Operational Services  Laura Roddan, Manager of Planning Services  Ryan Thoms, Manager of Emergency Services  Jason Kouwenhoven, Manager of Environmental Services  Melanie Taylor, Occupational Health & Safety Coordinator  Sarah West, Assistant Manager of Administrative Services  Cherise Roberts, Planner  Sherry Lawson, Records Management Clerk      1.  CALL TO ORDER  Chair Brander called the meeting to order 3:30 pm.       Minutes \\u2013 Regional Board   April 3, 2024   2.  APPROVAL OF AGENDA    2.1  Regional Board Agenda    Moved and Seconded\",\n",
      "  \"categories\": [\n",
      "    \"other__emergency_management\",\n",
      "    \"other__geographically-oriented\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"mental_health\",\n",
      "    \"youth_children\",\n",
      "    \"other__emergency_management\",\n",
      "    \"healthy_built_environments__community_design/planning\"\n",
      "  ],\n",
      "  \"id\": 0\n",
      "}\n",
      "{\n",
      "  \"text\": \"the agenda be , as presented.  MOTION CARRIED. 3.  ADOPTION OF MINUTES\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"healthy_built_environments\",\n",
      "    \"other__emergency_management\",\n",
      "    \"physical_activity\",\n",
      "    \"indigenous_relations\",\n",
      "    \"children___youth\",\n",
      "    \"mental_wellness__social_connectedness\"\n",
      "  ],\n",
      "  \"id\": 1\n",
      "}\n",
      "{\n",
      "  \"text\": \"4.  DELEGATION & INQUIRIES  5.  UNFINISHED BUSINESS  6.  CORRESPONDENCE    6.1  Correspondence dated February 29, 2024 from Mayor Sue McKortoff,  Town of Osoyoos re Support for Resolution by UBCM Members\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"indigenous_relations__reconciliation\"\n",
      "  ],\n",
      "  \"id\": 2\n",
      "}\n",
      "{\n",
      "  \"text\": \"7.  BUSINESS ARISING FROM CORRESPONDENCE  8.  CONSENT AGENDA  At the request of Chair Brander, item 8.2 Texada Island (Gillies Bay) Airport Hangar  Lease Update was removed from the Consent Agenda and placed under the heading\",\n",
      "  \"categories\": [\n",
      "    \"housing__permits\",\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 3\n",
      "}\n",
      "{\n",
      "  \"text\": \"Resolutions Removed from Consent Agenda as item 9.1. Moved and Seconded\",\n",
      "  \"categories\": [\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing__permits\",\n",
      "    \"housing\",\n",
      "    \"climate_change\",\n",
      "    \"children___youth__early_childhood_development\",\n",
      "    \"indigenous_relations__reconciliation\"\n",
      "  ],\n",
      "  \"id\": 4\n",
      "}\n",
      "{\n",
      "  \"text\": \"the following items on the Consent Agenda be :    8.1  Correspondence dated February 27, 2024 from Abby McLennan, qathet  Shoreline Cleanup Project Manager, Let's Talk Trash (LTT) in Partnership  with the Ocean Legacy Foundation re Letter of Support Request the Board provide Let\\u2019s Talk Trash and the Ocean Legacy Foundation a  letter of support for their 2024 BC Clean Coast Clean Waters Initiative  application to conduct shoreline cleanup efforts within the region.  MOTION CARRIED.\",\n",
      "  \"categories\": [\n",
      "    \"housing__permits\",\n",
      "    \"indigenous_relations\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"environmental_exposures__liquid_waste/_wastewater/sewage\",\n",
      "    \"environmental_exposures__air_quality_\",\n",
      "    \"housing__permits\",\n",
      "    \"healthy_built_environments__natural_environments/green_infrastructure\",\n",
      "    \"indigenous_relations__decolonization\",\n",
      "    \"indigenous_relations__reconciliation\",\n",
      "    \"healthy_built_environments\",\n",
      "    \"indigenous_relations\"\n",
      "  ],\n",
      "  \"id\": 5\n",
      "}\n",
      "{\n",
      "  \"text\": \"8.3  Review of Rural Housing Solutions Final Report the Board enter into the minutes and file the report titled \\u201cReview of  Rural Housing Solutions Final Report\\u201d dated March 20, 2024.  MOTION CARRIED.\",\n",
      "  \"categories\": [\n",
      "    \"housing__homelessness\",\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"housing__permits\",\n",
      "    \"housing__homelessness\"\n",
      "  ],\n",
      "  \"id\": 6\n",
      "}\n",
      "{\n",
      "  \"text\": \"Page 2 of 4 Minutes \\u2013 Regional Board   April 3, 2024   9.  RESOLUTIONS REMOVED FROM CONSENT AGENDA\",\n",
      "  \"categories\": [\n",
      "    \"housing__permits\",\n",
      "    \"other__geographically-oriented\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing__permits\",\n",
      "    \"climate_change\",\n",
      "    \"housing\",\n",
      "    \"youth_children\"\n",
      "  ],\n",
      "  \"id\": 7\n",
      "}\n",
      "{\n",
      "  \"text\": \"9.1  Texada Island (Gillies Bay) Airport Hangar Lease Update    Moved and Seconded\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"physical_activity\"\n",
      "  ],\n",
      "  \"id\": 8\n",
      "}\n",
      "{\n",
      "  \"text\": \"the Committee recommends to the Board the Board approve the\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing__permits\",\n",
      "    \"housing\",\n",
      "    \"indigenous_relations__reconciliation\",\n",
      "    \"mental_health\",\n",
      "    \"other__emergency_management\"\n",
      "  ],\n",
      "  \"id\": 9\n",
      "}\n",
      "{\n",
      "  \"text\": \"new lease format and adopt Option 2 in the Financial Impact section of the\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"id\": 10\n",
      "}\n",
      "{\n",
      "  \"text\": \"Texada Island (Gillies Bay) Airport Hangar Lease Update Report dated March  5, 2024, to raise the lease rate to $0.09 (9 cents) per square foot.  MOTION DEFEATED.\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"id\": 11\n",
      "}\n",
      "{\n",
      "  \"text\": \"Opposed (6): Director Lennox, Director Gisborne, Director McCormick, Director Fall,  Director Elliott, and Alternate Director Southcott.  Cherise Roberts joined the meeting at 3:40 pm.  Moved and Seconded\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"indigenous_relations\",\n",
      "    \"indigenous_relations__reconciliation\",\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"housing__permits\",\n",
      "    \"equity_and_diversity__racism\"\n",
      "  ],\n",
      "  \"id\": 12\n",
      "}\n",
      "{\n",
      "  \"text\": \"the Board refer the draft hangar lease agreement to the Texada Airport  Advisory Committee meeting of April 19 before consideration of this item at the  April Committee of the Whole meeting.  MOTION CARRIED. 10.  RESOLUTIONS\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 13\n",
      "}\n",
      "{\n",
      "  \"text\": \"11.  REPORTS  12.  BYLAWS  13.  NEW BUSINESS  14.  QUESTION PERIOD  15.  IN CAMERA SESSION    15.1 Proposed Closed Session  Michelle Jones, Corporate Officer requested item (k) be inserted into the proposed  closed session recommendation. Moved and Seconded\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing__permits\",\n",
      "    \"other__emergency_management\",\n",
      "    \"housing\",\n",
      "    \"indigenous_relations\",\n",
      "    \"youth_children\"\n",
      "  ],\n",
      "  \"id\": 14\n",
      "}\n",
      "{\n",
      "  \"text\": \"the Board move in-camera and the meeting be closed to the public  on the grounds the subject matter to be considered relates to matters  covered by the Community Charter under section 90(1):     (c) labour relations or other employee relations;        Page 3 of 4 Minutes \\u2013 Regional Board   April 3, 2024   (k) negotiations and related discussions respecting the proposed provision of a  regional district service are at their preliminary stages and , in the view  of the Board, could reasonably be expected to harm the interests of the  regional district if they were held in public;     (l) discussions with regional district officers and employees respecting regional  district objectives, measures and progress reports for the purposes of  preparing an annual report under section 98; and     (n) the consideration of whether a Board meeting should be closed under a  provision of this subsection or subsection(2).  MOTION CARRIED. The meeting went in-camera at 3:43 pm.  The open meeting resumed at 3:47 pm.  16.  RISE AND REPORT FROM IN CAMERA SESSION  17.  ADJOURNMENT  There being no further business, the meeting adjourned at 3:48 pm.  Chair  Corporate Officer    Page 4 of 4\",\n",
      "  \"categories\": [\n",
      "    \"harm_reduction\",\n",
      "    \"housing\",\n",
      "    \"other__geographically-oriented\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"mental_health\",\n",
      "    \"youth_children\",\n",
      "    \"housing__permits\",\n",
      "    \"other__emergency_management\"\n",
      "  ],\n",
      "  \"id\": 15\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# NLP\n",
    "pdf_folder = 'data/batch/'\n",
    "pdf_file = metting_name + \".pdf\"\n",
    "pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "# get the info of the meeting\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "context = {}\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "context['pdf'] = text\n",
    "# get the metadata\n",
    "metadata = fitz.open(pdf_path).metadata\n",
    "context['metadata'] = metadata\n",
    "# check if there is a txt with the same name\n",
    "txt_path = os.path.join(pdf_folder, pdf_file.replace('.pdf', '.txt'))\n",
    "if os.path.exists(txt_path):\n",
    "    with open(txt_path, 'r') as file:\n",
    "        context['transcript'] = file.read()\n",
    "\n",
    "else:\n",
    "        context['transcript'] = None\n",
    "\n",
    "df = pd.read_excel(\"data/themekeywordmap.xlsx\")\n",
    "category_keywords = {}\n",
    "for theme, group in df.groupby('theme'):\n",
    "    category_keywords[theme] = group['phrase'].tolist()\n",
    "\n",
    "split_patterns = [\n",
    "    r\"MOVED by\",\n",
    "    r\"SECONDED by\",\n",
    "    r\"WHEREAS\",\n",
    "    r\"THEREFORE BE IT RESOLVED THAT\",\n",
    "    r\"CARRIED UNANIMOUSLY\",\n",
    "    r\"REJECTED\",\n",
    "    r\"THAT\",\n",
    "    r\"APPROVED\",\n",
    "    r\"ADOPTED\",\n",
    "    r\"RESOLVED\"\n",
    "]\n",
    "Proposal_indicators = [\"Moved\", \"Seconded\", \"Motion\", \"Carried\", \"Proposal\", \"Passed\", \"Adopted\", \"Adoption\", \"Rejected\", \"Lost\", \"Moved\", \"approve\", \"Seconded\" , \"Adopt\", \"Resolution\", \"rejected\", \"Ordinance\", \"defeated\", \"discussed\", \"withdrawn\", \"tabled\", \"Amendment\", \"Amendment\", \"Recommendation\", \"granted\", \"Petition\", \"denied\", \"Vote\", \"result\"]\n",
    "# lower case it\n",
    "Proposal_indicators = [x.lower() for x in Proposal_indicators]\n",
    "segments = segment_document(context['pdf'], split_patterns)\n",
    "combined_segments = combine_segments_v3(segments, category_keywords, Proposal_indicators)\n",
    "len(combined_segments)\n",
    "\n",
    "# Load the FAISS index and category indices\n",
    "index = faiss.read_index('data/faiss_index.bin')\n",
    "category_indices = np.load('data/category_indices.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# Verify categories and update segments\n",
    "for segment in combined_segments:\n",
    "    suggested_categories = query_vector_database(segment['text'], model, index, category_indices)\n",
    "    segment[\"categories_by_vd\"] = suggested_categories\n",
    "    segment[\"id\"] = combined_segments.index(segment)\n",
    "\n",
    "# Print the updated categorized segments\n",
    "for segment in combined_segments:\n",
    "    print(json.dumps(segment, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a utility function to get the dictionary from combined_segments with specific id\n",
    "# combined_segments = [{'id': <id1>, 'txt': <text 1>, 'category': <category 1>, \"category_by_vd\":< cat1>\"},\n",
    "#  {'id': <id2>, 'txt': <text 2>, 'category': <category 2>, \"category_by_vd\":< cat2>\"}, ...]\n",
    "\n",
    "def get_dict_by_id(combined_segments, id):\n",
    "    for segment in combined_segments:\n",
    "        if segment[\"id\"] == id:\n",
    "            return segment\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "### State\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        sprop: list of different suggested proposal dictionaries with key id, cat, prop by NER\n",
    "        last_id: last visited ID\n",
    "        count: number of unique proposal\n",
    "        suggested_proposal: suggested_proposal\n",
    "        output: dictionary of features and adjusted poposal\n",
    "        document: list of related document\n",
    "    \"\"\"\n",
    "    sprop: List[dict]\n",
    "    last_id: int\n",
    "    count: int\n",
    "    suggested_proposal: str\n",
    "    output: dict\n",
    "    document: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "def iterate(state):\n",
    "    \"\"\"\n",
    "    Iterate over the sprop dictionary and return the last_id.\n",
    "    For the first time, call the iterate with input last_id as -1.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, sprop, that contains the next suggested proposal\n",
    "    \"\"\"\n",
    "    print(\"-------************ITERATE NODE***************-------\")\n",
    "    last_id = state[\"last_id\"]\n",
    "    last_id += 1\n",
    "    print(\"running for id: \", last_id)\n",
    "    \n",
    "    return {\"last_id\": last_id}\n",
    "\n",
    "def proposal_count(state):\n",
    "    \"\"\"\n",
    "    Determines whether the suggested_proposal is actually a proposal.\n",
    "    update the count state\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): change the count of the proposal\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---PROPOSAL COUNT NODE---\")\n",
    "    suggested_proposal = state[\"sprop\"][state[\"last_id\"]][\"text\"]\n",
    "    category = get_dict_by_id(state[\"sprop\"], state[\"last_id\"])[\"categories_by_vd\"]\n",
    "    \n",
    "    score = retrieval_counter.invoke({\"category\": category, \"suggested_proposal\": suggested_proposal})\n",
    "        \n",
    "    count = score[\"count\"]\n",
    "    return {\"count\": count,  \"suggested_proposal\": suggested_proposal}\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE NODE---\")\n",
    "    suggested_proposal = state[\"suggested_proposal\"]\n",
    "\n",
    "    # Retrieval\n",
    "    document = retriever.invoke(suggested_proposal)[:3]\n",
    "    document = [d.page_content for d in document]\n",
    "    return {\"document\": document}\n",
    "\n",
    "\n",
    "\n",
    "def get_proposal(state):\n",
    "    \"\"\"\n",
    "    Get all the suggested proposal from the suggested proposal.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, suggested_proposal, that contains the suggested proposal\n",
    "    \"\"\"\n",
    "    print(\"---GET PROPOSAL NODE---\")\n",
    "    all_proposals = state[\"suggested_proposal\"]\n",
    "    count = state[\"count\"]\n",
    "\n",
    "\n",
    "    prop_dict = retrieval_prop.invoke({\"all_proposals\": all_proposals, \"count\": count})\n",
    "    new_prop = prop_dict.values()\n",
    "    # create dic with format state[\"sprop\"] with id, text, categories, categories_by_vd\n",
    "    sprop = []\n",
    "    old_prop = get_dict_by_id(state[\"sprop\"], state[\"last_id\"])\n",
    "    \n",
    "    for i, prop in enumerate(new_prop):\n",
    "        sprop.append({\"id\": len(state[\"sprop\"])+i, \n",
    "                      \"text\": prop, \n",
    "                      \"categories\":old_prop[\"categories\"] , \n",
    "                      \"categories_by_vd\": old_prop[\"categories_by_vd\"]})\n",
    "    \n",
    "    state[\"sprop\"].extend(sprop)\n",
    "    return {\"sprop\": state[\"sprop\"]}\n",
    "\n",
    "\n",
    "\n",
    "def proposal_boundary(state):\n",
    "    \"\"\"\n",
    "    Determines the currect bounday of the suggested_proposal.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): change the count of the proposal\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ADJUST BOUNDARY NODE---\")\n",
    "    suggested_proposal = state[\"suggested_proposal\"]\n",
    "    document = state[\"document\"]\n",
    "    boundry = retrieval_boundary.invoke({\"document\": document, \"suggested_proposal\": suggested_proposal})\n",
    "    proposal_adj = adjusted_proposal_boundary(document, suggested_proposal, boundry)\n",
    "    state[\"output\"] = {\"full proposal\" :proposal_adj}\n",
    "    return { \"suggested_proposal\": suggested_proposal, \"document\": document, \"output\": state[\"output\"]}\n",
    "\n",
    "\n",
    "def proposal_features(state):\n",
    "    \"\"\"\n",
    "    Extract the features of the proposal.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): change the count of the proposal\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---EXTRACT FEATURES NODE---\")\n",
    "    suggested_proposal = state[\"suggested_proposal\"]\n",
    "    prop = get_dict_by_id(state[\"sprop\"], state[\"last_id\"])\n",
    "    document = state[\"document\"][:1]\n",
    "    result = chain_feature.invoke({'document':document ,'suggested_proposal': suggested_proposal, 'category': prop[\"categories_by_vd\"]})\n",
    "    # convert ProposalFeatures to dict\n",
    "    state[\"output\"].update(result.dict()) \n",
    "\n",
    "    # save to the final dict\n",
    "    final_dict[state[\"last_id\"]] = {k:v for k,v in state[\"output\"].items() if k != \"category_llm\"}\n",
    "    prop[\"categories_by_vd\"].extend(prop[\"categories\"])\n",
    "    # check if state[\"output\"][\"category_llm\"] is a list\n",
    "    if type(state[\"output\"][\"category_llm\"]) == list:\n",
    "        prop[\"categories_by_vd\"].extend(state[\"output\"][\"category_llm\"])\n",
    "    else:\n",
    "        prop[\"categories_by_vd\"].append(state[\"output\"][\"category_llm\"])\n",
    "\n",
    "    final_dict[state[\"last_id\"]]['theme'] = list(set(prop[\"categories_by_vd\"]))\n",
    "\n",
    "    return { \"suggested_proposal\": suggested_proposal, \"document\": document, \"output\": state[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conditional edge\n",
    "def route_count(state):\n",
    "    \"\"\"\n",
    "    Route based on the value of count.\n",
    "    0: Invalid proposal, go to iterate node\n",
    "    1: Valid proposal, go to retrieve node\n",
    "    >1: More than one proposal, go to get_suggested_proposal node\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTING based on validity---\")\n",
    "    grade = state[\"count\"]\n",
    "    if grade == 0:\n",
    "        print(\"Suggested proposal is NOT valid. Moving to the next one ... \")\n",
    "        return \"iterate\"\n",
    "    elif grade > 1:\n",
    "        print(\"Suggested proposal is MORE than one. Calling get_proposal ...\")\n",
    "        return \"get_proposal\"\n",
    "    else:\n",
    "        print(\"Suggested proposal is valid. retrieve the document ...\")\n",
    "        return \"retrieve\"\n",
    "    \n",
    "\n",
    "def stop_iteration(state):\n",
    "    \"\"\"\n",
    "    Stop the iteration if all the suggested proposal is visited.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK STOP ITERATION---\")\n",
    "    last_id = state[\"last_id\"]\n",
    "    if last_id == len(state[\"sprop\"]):\n",
    "        print(\"STOPPED!\")\n",
    "        return \"END\"\n",
    "    print(\"Continue ...\")\n",
    "    return \"proposal_count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"iterate\", iterate)  # iterate loop\n",
    "workflow.add_node(\"proposal_count\", proposal_count)  # count the proposal\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve related documents\n",
    "workflow.add_node(\"get_proposal\", get_proposal)  # get new proposal\n",
    "workflow.add_node(\"proposal_boundary\", proposal_boundary)  # adjust proposal boundary\n",
    "workflow.add_node(\"proposal_features\", proposal_features)  # extract proposal features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph build\n",
    "\n",
    "workflow.set_entry_point(\"iterate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"iterate\",\n",
    "    stop_iteration,\n",
    "    {\n",
    "        \"proposal_count\": \"proposal_count\",\n",
    "        \"END\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"proposal_count\",\n",
    "    route_count,\n",
    "    {\n",
    "        \"iterate\": \"iterate\",\n",
    "        \"get_proposal\": \"get_proposal\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"get_proposal\", \"iterate\")\n",
    "workflow.add_edge(\"retrieve\", \"proposal_boundary\")\n",
    "workflow.add_edge(\"proposal_boundary\", \"proposal_features\")\n",
    "workflow.add_edge(\"proposal_features\", \"iterate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------************ITERATE NODE***************-------\n",
      "running for id:  0\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is NOT valid. Moving to the next one ... \n",
      "'Finished running: proposal_count'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  1\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  2\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  3\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  4\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  5\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  6\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  7\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  8\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  9\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  10\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is NOT valid. Moving to the next one ... \n",
      "'Finished running: proposal_count'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  11\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  12\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is NOT valid. Moving to the next one ... \n",
      "'Finished running: proposal_count'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  13\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  14\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is NOT valid. Moving to the next one ... \n",
      "'Finished running: proposal_count'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  15\n",
      "---CHECK STOP ITERATION---\n",
      "STOPPED!\n",
      "'Finished running: iterate'\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "inputs = {\"sprop\": combined_segments, \"last_id\": -1} #-1\n",
    "for output in app.stream(inputs,  {\"recursion_limit\": 1000}):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert final_dict to csv file with keys as row and each key of the value as column\n",
    "# it should look like this\n",
    "# meeting name, id ,title, full proposal, theme, vote_result, future_date\n",
    "import pandas as pd\n",
    "# first load the csv output if exists.\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(final_dict, orient='index')\n",
    "df[\"meeting_name\"] = metting_name   \n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={\"index\": \"id\"})\n",
    "df = df[[\"meeting_name\", \"id\", \"title\", \"full proposal\", \"theme\", \"vote_result\", \"future_date\"]]\n",
    "# replace None values with \"\"\n",
    "df = df.replace({None: \"NONE\"})\n",
    "\n",
    "# append df0 to end of df if meeting_name is not in df0, otherwize replace the row with the same meeting_name and append the rest\n",
    "\n",
    "try:\n",
    "    df0 = pd.read_csv(\"data/output.csv\")\n",
    "    if metting_name in df0[\"meeting_name\"].unique():\n",
    "        # replace the row\n",
    "        df0 = df0[df0[\"meeting_name\"] != metting_name]\n",
    "        df = pd.concat([df, df0], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.concat([df, df0], ignore_index=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "df.to_csv(\"data/output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSIDERATIONS\n",
    "1. making sure the length of the inputs (category, context, ...) are limited\n",
    "2. use category_by_vd after the boundary detected\n",
    "3. come up with a preprocessing to reduce the chunks before feding to LLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
