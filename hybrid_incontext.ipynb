{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'gibsons',\n",
       " 'location type': 'municipality',\n",
       " 'meeting type': 'regular_council',\n",
       " 'data type': 'minutes',\n",
       " 'meeting date': Timestamp('2024-04-09 00:00:00'),\n",
       " 'transcript': 'Yes',\n",
       " 'comment': nan}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one file:\n",
    "pdf_folder = 'data/batch/'\n",
    "metting_name = \"gib_mcp_rgc_min__2024-04-09__01\"\n",
    "pdf_file = metting_name + \".pdf\"\n",
    "pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "# read the info from a xlsx file\n",
    "df = pd.read_excel(\"data/meetingmap.xlsx\")\n",
    "df = df.set_index('standard name')\n",
    "# get the info of the meeting\n",
    "meeting_info = df.loc[pdf_file]\n",
    "# return all the columns for the meeting\n",
    "meeting_info = meeting_info.to_dict()\n",
    "meeting_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# create a dictionary to store the text of each pdf and the metadata and if there is a txt with the same name\n",
    "context = {}\n",
    "    \n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "context['pdf'] = text\n",
    "# get the metadata\n",
    "metadata = fitz.open(pdf_path).metadata\n",
    "context['metadata'] = metadata\n",
    "# check if there is a txt with the same name\n",
    "txt_path = os.path.join(pdf_folder, pdf_file.replace('.pdf', '.txt'))\n",
    "if os.path.exists(txt_path):\n",
    "    with open(txt_path, 'r') as file:\n",
    "        context['transcript'] = file.read()\n",
    "\n",
    "else:\n",
    "        context['transcript'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pdf', 'metadata', 'transcript'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of themes:  56\n"
     ]
    }
   ],
   "source": [
    "# load the theme keywords from themekeywordmap.xlsx\n",
    "# convert it to a dictionary with the theme as key and different keywords as a list\n",
    "# it has two columns: theme and single phrase\n",
    "# combine all the phrases that have the same theme in  a list as follows\n",
    "# category_keywords = {\n",
    "#     \"theme 1\": [\"phrase 1\", \"phrase 2\", \"phrase 3\"],\n",
    "#     \"theme 2\": [\"phrase 4\", \"phrase 5\", \"phrase 6\", \"phrase 7\"]\n",
    "# }\n",
    "df = pd.read_excel(\"data/themekeywordmap.xlsx\")\n",
    "category_keywords = {}\n",
    "for theme, group in df.groupby('theme'):\n",
    "    category_keywords[theme] = group['phrase'].tolist()\n",
    "print(\"total number of themes: \", len(category_keywords.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Segment using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_patterns = [\n",
    "    r\"MOVED by\",\n",
    "    r\"SECONDED by\",\n",
    "    r\"WHEREAS\",\n",
    "    r\"THEREFORE BE IT RESOLVED THAT\",\n",
    "    r\"CARRIED UNANIMOUSLY\",\n",
    "    r\"REJECTED\",\n",
    "    r\"THAT\",\n",
    "    r\"APPROVED\",\n",
    "    r\"ADOPTED\",\n",
    "    r\"RESOLVED\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_document(document, patterns):\n",
    "    \"\"\"\n",
    "    Segment the document based on defined patterns.\n",
    "    \"\"\"\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "    segments = re.split(combined_pattern, document, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Filter out empty segments and strip whitespace\n",
    "    segments = [seg.strip() for seg in segments if seg.strip()]\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keywords(segment, category_keywords):\n",
    "    \"\"\"\n",
    "    Match segments against category keywords.\n",
    "    \"\"\"\n",
    "    matched_categories = []\n",
    "    for category, keywords in category_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', segment, re.IGNORECASE):\n",
    "                matched_categories.append(category)\n",
    "                break  # Break after the first match to avoid redundant checks\n",
    "    return matched_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segments(segments, category_keywords):\n",
    "    \"\"\"\n",
    "    Combine nearby segments and filter them based on category keywords.\n",
    "    \"\"\"\n",
    "    combined_segments = []\n",
    "    current_segment = \"\"\n",
    "    current_categories = set()\n",
    "\n",
    "    for segment in segments:\n",
    "        matched_categories = match_keywords(segment, category_keywords)\n",
    "        if matched_categories:\n",
    "            if current_segment:\n",
    "                combined_segments.append({\n",
    "                    \"text\": current_segment,\n",
    "                    \"categories\": list(current_categories)\n",
    "                })\n",
    "                current_segment = segment\n",
    "                current_categories = set(matched_categories)\n",
    "            else:\n",
    "                current_segment = segment\n",
    "                current_categories.update(matched_categories)\n",
    "        else:\n",
    "            current_segment += \" \" + segment\n",
    "    \n",
    "    if current_segment:\n",
    "        combined_segments.append({\n",
    "            \"text\": current_segment,\n",
    "            \"categories\": list(current_categories)\n",
    "        })\n",
    "\n",
    "    return combined_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = segment_document(context['pdf'], split_patterns)\n",
    "combined_segments = combine_segments(segments, category_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total initial segments: 8\n"
     ]
    }
   ],
   "source": [
    "# initial assesment results\n",
    "print(\"Total initial segments:\", len(combined_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_nlp = json.dumps(combined_segments, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_segments_v2(segments, category_keywords):\n",
    "    \"\"\"\n",
    "    Combine nearby segments and filter them based on category keywords.\n",
    "    \"\"\"\n",
    "    combined_segments = []\n",
    "    current_segment = \"\"\n",
    "    current_categories = set()\n",
    "    last_matched_categories = set()\n",
    "\n",
    "    for segment in segments:\n",
    "        matched_categories = match_keywords(segment, category_keywords)\n",
    "        if matched_categories:\n",
    "            # If current segment is not empty and the new segment has different categories,\n",
    "            # add the current segment to combined_segments and start a new one\n",
    "            if current_segment and matched_categories != last_matched_categories:\n",
    "                combined_segments.append({\n",
    "                    \"text\": current_segment,\n",
    "                    \"categories\": list(current_categories)\n",
    "                })\n",
    "                current_segment = segment\n",
    "                current_categories = set(matched_categories)\n",
    "            else:\n",
    "                current_segment += \" \" + segment\n",
    "                current_categories.update(matched_categories)\n",
    "            last_matched_categories = matched_categories\n",
    "        else:\n",
    "            current_segment += \" \" + segment\n",
    "    \n",
    "    # Append the last segment\n",
    "    if current_segment:\n",
    "        combined_segments.append({\n",
    "            \"text\": current_segment,\n",
    "            \"categories\": list(current_categories)\n",
    "        })\n",
    "\n",
    "    return combined_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proposal_indicators = [\"Moved\", \"Seconded\", \"Motion\", \"Carried\", \"Proposal\", \"Passed\", \"Adopted\", \"Adoption\", \"Rejected\", \"Lost\", \"Moved\", \"approve\", \"Seconded\" , \"Adopt\", \"Resolution\", \"rejected\", \"Ordinance\", \"defeated\", \"discussed\", \"withdrawn\", \"tabled\", \"Amendment\", \"Amendment\", \"Recommendation\", \"granted\", \"Petition\", \"denied\", \"Vote\", \"result\"]\n",
    "# lower case it\n",
    "Proposal_indicators = [x.lower() for x in Proposal_indicators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_segments_v3(segments, category_keywords, Proposal_indicators):\n",
    "    combined_proposals = []\n",
    "    temp_segment = \"\"\n",
    "\n",
    "    def contains_proposal_indicators(text):\n",
    "            return any(indicator in text.lower() for indicator in Proposal_indicators)\n",
    "    def match_keywords(segment, category_keywords):\n",
    "        \"\"\"\n",
    "        Match segments against category keywords.\n",
    "        \"\"\"\n",
    "        matched_categories = []\n",
    "        for category, keywords in category_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if re.search(r'\\b' + re.escape(keyword) + r'\\b', segment, re.IGNORECASE):\n",
    "                    matched_categories.append(category)\n",
    "        return matched_categories\n",
    "  \n",
    "    \n",
    "    for seg in segments:\n",
    "        \"\"\"Splits/combine the segments such that it contains one proposals indicator.\"\"\"\n",
    "        for line in seg.split('\\n'):\n",
    "            temp_segment += line + \" \"\n",
    "            if contains_proposal_indicators(line):\n",
    "                categories = match_keywords(temp_segment, category_keywords)\n",
    "                combined_proposals.append({\n",
    "                    \"text\": temp_segment.strip(),\n",
    "                    \"categories\": list(set(categories))\n",
    "                })\n",
    "                temp_segment = \"\"\n",
    "                category_set = set()\n",
    "    if temp_segment.strip():\n",
    "        # append it to the last proposal\n",
    "        categories = match_keywords(temp_segment, category_keywords)\n",
    "        combined_proposals[-1] = {\n",
    "                    \"text\": combined_proposals[-1][\"text\"] + \" \" + temp_segment.strip(),\n",
    "                    \"categories\": list(set(categories+ combined_proposals[-1][\"categories\"])),\n",
    "                }\n",
    "    for i in reversed(range(len(combined_proposals))):\n",
    "        if len(combined_proposals[i][\"text\"]) < 50:\n",
    "            combined_proposals[i-1][\"text\"] = combined_proposals[i-1][\"text\"] + \" \" + combined_proposals[i][\"text\"]\n",
    "            # remove the current segment\n",
    "            del combined_proposals[i]\n",
    "    return combined_proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_segments = combine_segments(segments, category_keywords)\n",
    "#combined_segments = combine_segments_v2(segments, category_keywords)\n",
    "combined_segments = combine_segments_v3(segments, category_keywords, Proposal_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '2024 Councillor Thompson Councillor Croal Development Permit Delegation Authority Amendment Bylaw No.1054- 04, 2024 be .  CARRIED',\n",
       " 'categories': ['housing']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_segments[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: find category based on vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Generate embeddings for each category based on keywords\n",
    "category_embeddings = {}\n",
    "for category, keywords in category_keywords.items():\n",
    "    category_embeddings[category] = model.encode(keywords, convert_to_tensor=True)\n",
    "\n",
    "# Flatten category embeddings for FAISS indexing\n",
    "flat_embeddings = []\n",
    "category_indices = []\n",
    "for category, embeddings in category_embeddings.items():\n",
    "    for embedding in embeddings:\n",
    "        flat_embeddings.append(embedding.cpu().detach().numpy())\n",
    "        category_indices.append(category)\n",
    "\n",
    "flat_embeddings = np.vstack(flat_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = flat_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(flat_embeddings)\n",
    "\n",
    "# Save FAISS index and data for later use\n",
    "faiss.write_index(index, 'data/faiss_index.bin')\n",
    "np.save('data/category_indices.npy', category_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_elbow_point_indices(data):\n",
    "    # Sort the data and keep track of the original indices\n",
    "    sorted_data_with_indices = sorted((val, idx) for idx, val in enumerate(data))\n",
    "    sorted_data = [val for val, idx in sorted_data_with_indices]\n",
    "    sorted_indices = [idx for val, idx in sorted_data_with_indices]\n",
    "    \n",
    "    # Calculate the differences between consecutive elements\n",
    "    differences = np.diff(sorted_data)\n",
    "    \n",
    "    # Find the index where the difference significantly increases\n",
    "    elbow_index = np.argmax(differences)\n",
    "    \n",
    "    # Find the elbow point value\n",
    "    elbow_point = sorted_data[elbow_index]\n",
    "    \n",
    "    # Find indices of elements to remove (smaller than or equal to the elbow point)\n",
    "    to_remove_indices = [idx for idx, val in enumerate(data) if val <= elbow_point]\n",
    "    return to_remove_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vector_database(text, model, index, category_indices,num_categories_to_search=20):\n",
    "    \"\"\"\n",
    "    Query the FAISS index with a text embedding and return the most relevant categories.\n",
    "    Avoid returning repetitive categories and apply a similarity threshold.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the text\n",
    "    text_embedding = model.encode([text], convert_to_tensor=True)\n",
    "    text_embedding = text_embedding.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    # Search the index for the most similar embeddings\n",
    "    distances, indices = index.search(text_embedding, num_categories_to_search)\n",
    "    to_remove = find_elbow_point_indices(distances[0])\n",
    "\n",
    "    # Filter out categories based on the threshold and avoid repetitions\n",
    "    seen_categories = set()\n",
    "    categories_by_vd = []\n",
    "    for idx in  indices[0]:\n",
    "        if idx in to_remove:\n",
    "            continue\n",
    "        category = category_indices[idx]\n",
    "        if category not in seen_categories:\n",
    "            categories_by_vd.append(category)\n",
    "            seen_categories.add(category)\n",
    "\n",
    "    \n",
    "\n",
    "    return categories_by_vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index and category indices\n",
    "index = faiss.read_index('data/faiss_index.bin')\n",
    "category_indices = np.load('data/category_indices.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# Verify categories and update segments\n",
    "for segment in combined_segments:\n",
    "    suggested_categories = query_vector_database(segment['text'], model, index, category_indices)\n",
    "    segment[\"categories_by_vd\"] = suggested_categories\n",
    "    segment[\"id\"] = combined_segments.index(segment)\n",
    "\n",
    "# Print the updated categorized segments\n",
    "for segment in combined_segments:\n",
    "    print(json.dumps(segment, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Regular Council Meeting Minutes - Tuesday, April 9, 2024    R2024-71  Support for Community Emergency Preparedness Fund (CEPF) Councillor De Andrade Councillor Croal Council supports the Sunshine Coast Regional District applying for,  receiving, and managing Community Emergency Preparedness Fund (CEPF)  Evacuation Route Planning grant funding on behalf of the Town of Gibsons.  CARRIED',\n",
       " 'categories': ['housing',\n",
       "  'environmental_exposures__extreme_weather',\n",
       "  'injury_prevention',\n",
       "  'other__geographically-oriented',\n",
       "  'other__emergency_management'],\n",
       " 'categories_by_vd': ['housing',\n",
       "  'environmental_exposures__extreme_weather',\n",
       "  'injury_prevention',\n",
       "  'youth_children',\n",
       "  'other__emergency_management',\n",
       "  'mental_health',\n",
       "  'injury_prevention__youth_self-harm',\n",
       "  'environmental_exposures__air_quality_'],\n",
       " 'id': 'gib_mcp_rgc_min__2024-04-09__01|10'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_segments[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3: lang-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LANGCHAIN_API_KEY from the environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Index\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the context['pdf'] and create vector store\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=10\n",
    ")\n",
    "def filter_none_values(metadata):\n",
    "    return {k: v for k, v in metadata.items() if v is not None}\n",
    "filtered_metadata = filter_none_values(context['metadata'])\n",
    "\n",
    "text_splits = text_splitter.split_text(context['pdf'])\n",
    "metadata_list = [filtered_metadata] * len(text_splits)\n",
    "\n",
    "# Add  text_splits to vectorDB with  nomic-embed-text-v1.5  and inference_mode=\"local\n",
    "vectorstore = Chroma.from_texts(\n",
    "        texts=text_splits,\n",
    "        metadatas=metadata_list,\n",
    "        # embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "        embedding=embeddings.OllamaEmbeddings(model=\"nomic-embed-text:v1.5\"),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_count = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are an assitance that detect the number of unique proposals with a specific categoty (theme) in a suggested proposal of a council meeting note. \\n\n",
    "You are provided with suggested proposal and its category as user prompt. \\n\n",
    "The goal is to find count of unique proposal in the suggested proposal with the specific category. \\n\n",
    "Give a integer count of unique proposal as a JSON with single key 'count'. \\n\n",
    "\n",
    "example:\n",
    "proposal: ### Minutes of the Council meeting of February 6, 2024, be approved.\\n CARRIED UNANIMOUSLY \\n Council Meeting\\n Minutes, February 27, 2024 3\\n 3. Council (City Finance and Services) \\n MOVED by Councillor Dominato\\n SECONDED by Councillor Carr\\n THAT the Minutes of the Council meeting following the Standing Committee on City\\n Finance and Services meeting of February 7, 2024, be approved.\\n CARRIED UNANIMOUSLY\\n 4. Court of Revision (Business Improvement Areas) - February 8, 2024\\n MOVED by Councillor Bligh\\n SECONDED by Councillor Zhou\\n  THAT the Minutes of the Court of Revision (Business Improvement Areas) meeting of\\n February 8, 2024, be approved.\\n CARRIED UNANIMOUSLY\\n MATTERS ADOPTED ON CONSENT \n",
    "categories: [\"City Finance and Services\", \"Business Improvement Areas\"]\n",
    "output: \"count\": 2\n",
    "\n",
    "Let's think step by step. Here are the steps to solve the task:\n",
    "1. Validity check: A proposal should suggest some action or decision and it should have a unique decision. \n",
    "2. Proposal Count: See if the suggested proposal includes more than one proposal. calculate the count.\n",
    "5. Write: Only return the count as integer in a JSON format with a single key 'count'.\n",
    "\n",
    "RULES:\n",
    "- your output MUST HAVE the exact JSON format.\n",
    "- Your answer must not include any speculation or inference. Do not assume or change dates and times. Only provide information that is explicitly stated in the context.\n",
    "- An answer is considered grounded if **all** information in **every** sentence in the answer is **explicitly** mentioned in the source context, **no** extra information is added and **no** inferred information is added.\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Category:\\n {category} \\n\\n\n",
    "    suggested_proposal:\\n {suggested_proposal} \\n\\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"category\", \"suggested_proposal\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 0}\n"
     ]
    }
   ],
   "source": [
    "num = 12\n",
    "suggested_proposal = combined_segments[num][\"text\"]\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "retrieval_counter = prompt_count | llm | JsonOutputParser()\n",
    "category = combined_segments[num][\"categories_by_vd\"]\n",
    "print(retrieval_counter.invoke({\"category\": category, \"suggested_proposal\": suggested_proposal}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def find_closest_match(original_string, substring):\n",
    "    # Initialize variables\n",
    "    closest_match_index = -1\n",
    "    highest_similarity = 0\n",
    "    substring_length = len(substring)\n",
    "    \n",
    "    # Define a function to calculate similarity ratio\n",
    "    def similarity(s1, s2):\n",
    "        return difflib.SequenceMatcher(None, s1, s2).ratio()\n",
    "    \n",
    "    # Compare the substring against all possible substrings of the same length in the original string\n",
    "    for i in range(len(original_string) - substring_length + 1):\n",
    "        current_substring = original_string[i:i + substring_length]\n",
    "        current_similarity = similarity(current_substring, substring)\n",
    "        if current_similarity > highest_similarity:\n",
    "            highest_similarity = current_similarity\n",
    "            closest_match_index = i\n",
    "    \n",
    "    return closest_match_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### Example:\n",
    "#### User Input:  \n",
    "# \"context\": \"The meeting was called to order at 10:00 AM. Moved by John Doe, the proposal to increase the budget for the community park was discussed. The committee deliberated on various aspects. Motion Carried. Moved by Tom. The agenda item to increase wages. Motion Carried. The next item\"\\n\n",
    "\"suggested_proposal\": \"increase the budget for the community park was discussed.\" \\n\\n\n",
    "#### Model correct Output:\n",
    "\"start\": \"Moved by John Doe, the proposal to increase\",\n",
    "\"end\": \" deliberated on various aspects. Motion Carried.\"\"\"\n",
    "\n",
    "prompt_boundry = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are an AI assistance to identify the adjusted boundary of a suggested_proposal within context (meeting notes). \\n\n",
    "You are given the context and the suggested proposal (a chunk of the context that is not accurately bounded). \\n\n",
    "Your task is to first locate the exact chunk (suggested_proposal) in the context, then add or remove words from it such that the new chunk include all related information about the proposal. \n",
    "Write the first and last 5 to 10 words of the context that is about the suggested proposal. Do NOT write about another proposal. Only one proposal should be in the boundary. \\n\n",
    "\n",
    "### Instructions:\n",
    "1. **Output Format**:\n",
    "    - Provide the output in JSON format with the keys 'start' and 'end'.\n",
    "    - Both 'start' and 'end' values should include atleast five and atmost ten words from the context. \n",
    "\n",
    "2. **Rules**:\n",
    "    - **Use the suggested proposal as the basis**: Ensure the adjusted boundary in the context indeed is same as the original suggested_proposal.\n",
    "    - Do not use speculations or inferences. Only provide information that is explicitly stated in the context and is about the suggested_proposal.\n",
    "    - Maintain the coherence and logical flow of the proposal.\n",
    "    - All necessary information such as the mover, the proposal title, the vote result, or note are considered related information and should be inside the boundary.\n",
    "    - ** word count**: DO NOT return less than 5 words or more than 10 words for each boundary marker 'start' and 'end'.\n",
    "    - only one proposal that is related to the suggested_proposal should be selected.\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "     Given the context and suggested proposal, follow the above instructions to determine the precise boundary. Ensure your output is in JSON with the required word count. Return ONLY and only one proposal. \\n\\n\n",
    "    \"context\":\\n {document} \\n\\n\n",
    "    \"suggested_proposal\":\\n {suggested_proposal} \\n\\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"document\", \"suggested_proposal\"],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 'The Mayor called the meeting to order at 7:00pm. APPROVAL OF THE AGENDA R2024-63 Regular Council Agenda - April 9, 2024 Councillor De Andrade Councillor Lumley the Regular Business Agenda of April 9, 2024 be adopted.', 'end': 'CARRIED ADOPTION OF MINUTES'}\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "suggested_proposal = combined_segments[num][\"text\"]\n",
    "\n",
    "llm_boundry = ChatOllama(model=local_llm, temperature=0, keep_alive=0)\n",
    "retrieval_boundary = prompt_boundry | llm_boundry | JsonOutputParser()\n",
    "txt_chunk = retriever.invoke(suggested_proposal)[:3]\n",
    "txt_chunk = [d.page_content for d in txt_chunk]\n",
    "boundry = retrieval_boundary.invoke({\"document\": txt_chunk, \"suggested_proposal\": suggested_proposal})\n",
    "print(boundry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular Council  MEETING MINUTES  Tuesday, April 9, 2024  Council Chambers, 7:00pm  Town Hall, 474 South Fletcher Road, Gibsons, BC      PRESENT:  Mayor Silas White   Councillor David Croal  Councillor Annemarie De Andrade  Councillor Stafford Lumley  Councillor Christi Thompson  Youth Representative Cael Read     STAFF:     Emanuel Machado, Chief Administrative Officer  Rebecca Anderson, Corporate Officer  Lorraine Coughlin, Director of Finance  Trevor Rutley, Director of Infrastructure Services  Lesley-Anne Staats, Director of Planning via Zoom   Noni Weitz, Manager of Financial Services  Heidi Siller, Executive Assistant (recorder)    CALL TO ORDER  The Mayor called the meeting to order at 7:00pm.    APPROVAL OF THE AGENDA      R2024-63  Regular Council Agenda - April 9, 2024 Councillor De Andrade Councillor Lumley the Regular Business Agenda of April 9, 2024 be .  CARRIED ADOPTION OF MINUTES'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ayor called the meeting to order at 7:00pm.    APPROVAL OF THE AGENDA      R2024-63  Regular Council Agenda - April 9, 2024  MOVED by Councillor De Andrade   SECONDED by Councillor Lumley    THAT the Regular Business Agenda of April 9, 2024 be adopted. CARRIED      ADOPTION OF MINUTES      R2024-64  Minutes of the Regul'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjusted_proposal_boundary(txt_chunk, proposal, boundry):\n",
    "    start = boundry[\"start\"]\n",
    "    end = boundry[\"end\"]\n",
    "    # check if the length of end or start is less 30\n",
    "    if len(start) < 30:\n",
    "        start = start + ' ' + end[:20] + ' ' + proposal[:20]\n",
    "    elif len(end) < 30:\n",
    "        end =end + ' ' +  proposal[-30:] + ' ' +  start[-30:] \n",
    "    txt = ''\n",
    "    for txts in txt_chunk:\n",
    "        txt += ' ' + txts.replace('\\n', ' ').strip()\n",
    "    start_index = find_closest_match(txt, start)\n",
    "    end_index = find_closest_match(txt, end)\n",
    "    # change the end_index to the closest . or ; or end of the sentence\n",
    "    end_index1 = txt.find(' ', end_index)\n",
    "    end_index2 = txt.find('.', end_index)\n",
    "    # find smaller positive index and set as end_index\n",
    "    if end_index1 >= 0 and end_index2 >= 0:\n",
    "        end_index = min(end_index1, end_index2)\n",
    "    elif end_index1 >=0:\n",
    "        end_index = end_index1\n",
    "    elif end_index2 >=0:\n",
    "        end_index = end_index2\n",
    "\n",
    "    return txt[start_index:end_index+len(end)]\n",
    "# get the sentence\n",
    "proposal_adj = adjusted_proposal_boundary(txt_chunk, suggested_proposal, boundry)\n",
    "\n",
    "proposal_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get suggested proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_getprop = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are an AI assistance to all the unique proposals from a all_proposals. \\n\n",
    "You are given the all_proposals: string  and  count:int from the user that shows how many proposals are in all_proposals.\\n\n",
    "Your task is to find different proposals from all_proposals such that each include all required information including proposer, title, results and etc. \n",
    "Output a JSON with keys as int and value as each individual proposal. \\n\n",
    "\n",
    "**Output Format**:\n",
    "    '1': <str, content of proposal 1>,\n",
    "    '2': <str, content of proposal 2>,\n",
    "    ...\n",
    "    'count': <str, content of proposal count>,\n",
    "\n",
    "### Instructions:\n",
    "1. **approach**: \n",
    "    - Find count number of unique proposals from all_proposals.\n",
    "    - Provide the output as a JSON with len equal to count and write unique proposals as values .\n",
    "\n",
    "2. **Rules**:\n",
    "    - **Use the all_proposals as the basis**: Ensure your individual proposals are indeed from  the original all_proposals.\n",
    "    - Do not use speculations or inferences. Only provide information that is explicitly stated in the all_proposals.\n",
    "    - Maintain the coherence and logical flow of the proposal.\n",
    "    - All necessary information such as the mover, the proposal title, the vote result, or note are considered related information and should be included for one proposal.\n",
    "    - ** word count**: DO NOT return less than 5 words or more than 20 words for proposals.\n",
    "\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "     Given the all_proposals and count, follow the above instructions to determine the individual proposals. Ensure your output is only JSON. size of JSON should be equal to count. Do NOT return anythings else other than the JSON. \\n\\n\n",
    "    \"all_proposals\":\\n {all_proposals} \\n\\n\n",
    "    \"count\":\\n {count} \\n\\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"all_proposals\", \"count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': '2024 Councillor Thompson Councillor Croal Development Permit Delegation Authority Amendment Bylaw No.1054-04, 2024 be . CARRIED COMMITTEE REPORTS Committee-of-the-Whole Meeting - March 19, 2024 The minutes of the Committee-of-the-Whole Meeting held March 19, 2024 were received.', '2': 'R2024-66 2024-2028 Preliminary General Services 5-Year Capital Plan Councillor Croal Councillor De Andrade the revised preliminary 5-year capital plan for general services be integrated into the 2024-2028 Financial Plan with the exception of the Dog Park and Pickleball projects which are to be removed from the capital plan and'}\n"
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "all_proposals = combined_segments[num][\"text\"] + ' ' + combined_segments[num+1][\"text\"]\n",
    "\n",
    "llm_getprop = ChatOllama(model=local_llm, temperature=0, keep_alive=0)\n",
    "retrieval_prop = prompt_getprop | llm_getprop | JsonOutputParser()\n",
    "prop_dict = retrieval_prop.invoke({\"all_proposals\": all_proposals, \"count\": 2})\n",
    "print(prop_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feaure extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "\n",
    "# Schema for structured response\n",
    "class ProposalFeatures(BaseModel):\n",
    "    title: str = Field(description=\"A short title of the proposal\", required=True)\n",
    "    category_llm: List[str] = Field(description=f\"category if the proposal choosen from suggested category by user\", required=True)\n",
    "    vote_result: str = Field(description=\"The result of the vote\", required=True)\n",
    "    future_date: Optional[str] = Field(description=\"The future date of the proposal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt_extraction =PromptTemplate(\n",
    "    template= \"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are an AI assistance that extract related information about a suggested_proposal from the context (meeting notes). \\n\n",
    "\n",
    "Instructuctions:\n",
    "The output should be in json format and structured with the following schema:\n",
    "\n",
    "    title: str = Field(description=\"A short title of the proposal\", required=True)\n",
    "    category_llm: List[str] = Field(description=f\"proposal category from Suggested_categories\", required=True)\n",
    "    vote_result: str = Field(description=\"The result of the vote\", required=True)\n",
    "    future_date: Optional[str] = Field(description=\"The future date of the proposal\")\n",
    "    \n",
    "RULES: \n",
    "1. Do not use speculations or inferences. You are given the Suggested_proposal and the context. every information should be only related to the Suggested_proposal from the context. \n",
    "2. The category_llm should be choosed from the suggested category by the user. It can be all or some of it that you think is related to the proposal. \n",
    "3. If you did not find any future date, return None.\n",
    "4. Your output should be in JSON format with the keys 'title', 'category_llm', 'vote_result', and 'future_date'. DO NOT change it.  \\n\n",
    "\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    \"context\":\\n {document} \\n\\n\n",
    "    \"suggested_proposal\":\\n {suggested_proposal} \\n\\n\n",
    "    \"Suggested_categories\":\\n {category} \\n\\n\n",
    "     Given the context, find the Suggested_proposal and return title, category_llm, vote_result, and future_date in a JSON formart as mentioned in the instruction and rules. DO NOT return any other key. Output should be of class ProposalFeatures.  \\n\\n\n",
    "     <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"document\", \"suggested_proposal\", \"category\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProposalFeatures(title='Parcel Tax Roll Review Panel – Water/Sewer/Community Recreation Parcel Taxes', category_llm=['environmental_exposures__liquid_waste/_wastewater/sewage', 'environmental_exposures__health_impact_assessments_/human_health_risk_assessments_/environmental_assessments'], vote_result='CARRIED', future_date='Tuesday, May 7, 2024')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 8\n",
    "suggested_proposal = combined_segments[num][\"text\"]\n",
    "category = combined_segments[num][\"categories_by_vd\"]\n",
    "txt_chunk = retriever.invoke(suggested_proposal)[:1]\n",
    "txt_chunk = [d.page_content for d in txt_chunk] \n",
    "llm_extraction  = OllamaFunctions(model=local_llm,format=\"json\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "structured_llm = llm_extraction.with_structured_output(ProposalFeatures)\n",
    "chain_feature = prompt_extraction | structured_llm\n",
    "\n",
    "result = chain_feature.invoke({'document':txt_chunk,'suggested_proposal': suggested_proposal, 'category': category})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver to the excel for all proposal\n",
    "# automatically nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langGraph control flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Regular Council  MEETING MINUTES  Tuesday, April 9, 2024  Council Chambers, 7:00pm  Town Hall, 474 South Fletcher Road, Gibsons, BC      PRESENT:  Mayor Silas White   Councillor David Croal  Councillor Annemarie De Andrade  Councillor Stafford Lumley  Councillor Christi Thompson  Youth Representative Cael Read     STAFF:     Emanuel Machado, Chief Administrative Officer  Rebecca Anderson, Corporate Officer  Lorraine Coughlin, Director of Finance  Trevor Rutley, Director of Infrastructure Services  Lesley-Anne Staats, Director of Planning via Zoom   Noni Weitz, Manager of Financial Services  Heidi Siller, Executive Assistant (recorder)    CALL TO ORDER  The Mayor called the meeting to order at 7:00pm.    APPROVAL OF THE AGENDA      R2024-63  Regular Council Agenda - April 9, 2024 Councillor De Andrade Councillor Lumley the Regular Business Agenda of April 9, 2024 be .  CARRIED ADOPTION OF MINUTES\",\n",
      "  \"categories\": [\n",
      "    \"youth_children\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"housing\",\n",
      "    \"healthy_built_environments__community_design/planning\",\n",
      "    \"youth_children\"\n",
      "  ],\n",
      "  \"id\": 0\n",
      "}\n",
      "{\n",
      "  \"text\": \"R2024-64  Minutes of the Regular Council Meeting - March 19, 2024 Councillor Croal Councillor De Andrade the minutes of the Regular Council meeting held March 19, 2024 be .  CARRIED\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"housing\",\n",
      "    \"youth_children\",\n",
      "    \"healthy_built_environments__active_transportation\",\n",
      "    \"indigenous_relations__reconciliation\"\n",
      "  ],\n",
      "  \"id\": 1\n",
      "}\n",
      "{\n",
      "  \"text\": \"Regular Council Meeting Minutes - Tuesday, April 9, 2024  BYLAWS      R2024-65  Development Permit Delegation Authority Amendment Bylaw No.1054-04,\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 2\n",
      "}\n",
      "{\n",
      "  \"text\": \"2024 Councillor Thompson Councillor Croal Development Permit Delegation Authority Amendment Bylaw No.1054- 04, 2024 be .  CARRIED\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"housing__permits\"\n",
      "  ],\n",
      "  \"id\": 3\n",
      "}\n",
      "{\n",
      "  \"text\": \"COMMITTEE REPORTS    Committee-of-the-Whole Meeting - March 19, 2024    The minutes of the Committee-of-the-Whole Meeting held March 19, 2024 were  received.        R2024-66  2024-2028 Preliminary General Services 5-Year Capital Plan Councillor Croal Councillor De Andrade the revised preliminary 5-year capital plan for general services be  integrated into the 2024-2028 Financial Plan with the exception of the Dog  Park and Pickleball projects which are to be removed from the capital plan and\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"healthy_built_environments__community_design/planning\",\n",
      "    \"other__emergency_management\"\n",
      "  ],\n",
      "  \"id\": 4\n",
      "}\n",
      "{\n",
      "  \"text\": \"referred to the 2025 Parks Master Plan for discussion;    AND the proposed budget for the Healthy Harbours Project be , with the 2024 portion being funded by accumulated surplus.  CARRIED\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"climate_change\",\n",
      "    \"poverty_affordability\",\n",
      "    \"healthy_built_environments__community_design/planning\",\n",
      "    \"healthy_built_environments__natural_environments/green_infrastructure\"\n",
      "  ],\n",
      "  \"id\": 5\n",
      "}\n",
      "{\n",
      "  \"text\": \"R2024-67  2024 Annual Tax Rates Councillor Lumley Councillor Croal the 2024 annual tax rates be prepared authorizing an overall 8% tax  increase (reflecting 3% for general operations and 5% for future policing  costs).  CARRIED\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"poverty_affordability\",\n",
      "    \"housing\",\n",
      "    \"youth_children\"\n",
      "  ],\n",
      "  \"id\": 6\n",
      "}\n",
      "{\n",
      "  \"text\": \"Regular Council Meeting Minutes - Tuesday, April 9, 2024  R2024-68  2024-2028 Financial Plan Bylaw and 2024 Annual Tax Rate Bylaw Councillor Lumley Councillor Croal the 2024-2028 Financial Plan Bylaw and the 2024 Annual Tax Rate  Bylaw be prepared for Council approval.  CARRIED\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"injury_prevention__youth_self-harm\"\n",
      "  ],\n",
      "  \"id\": 7\n",
      "}\n",
      "{\n",
      "  \"text\": \"ADMINISTRATION REPORTS    Budget Presentation     The budget presentation was received for information.         R2024-69  Parcel Tax Roll Review Panel \\u2013 Water/Sewer/Community Recreation  Parcel Taxes Councillor De Andrade Councillor Thompson Council convene a Parcel Tax Roll Review Panel at 6:30 pm on  Tuesday, May 7, 2024 in the Town\\u2019s Council chambers;    AND all members of Council be appointed to sit as members on the  Parcel Tax Roll Review Panel.  CARRIED\",\n",
      "  \"categories\": [\n",
      "    \"housing\",\n",
      "    \"physical_activity\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"youth_children\",\n",
      "    \"environmental_exposures__liquid_waste/_wastewater/sewage\",\n",
      "    \"environmental_exposures__health_impact_assessments_/_human_health_risk_assessments_/_environmental_assessments\"\n",
      "  ],\n",
      "  \"id\": 8\n",
      "}\n",
      "{\n",
      "  \"text\": \"CORRESPONDENCE      R2024-70  Canada BC Housing Benefit Program - Memorandum of Understanding Councillor Croal Councillor Thompson the Mayor and Corporate Officer be authorized to execute the Canada  BC Housing Benefit Program Memorandum of Understanding between the  Sunshine Resource Centre, Town of Gibsons and Sunshine Coast Affordable  Housing Society.  CARRIED\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"youth_children\"\n",
      "  ],\n",
      "  \"id\": 9\n",
      "}\n",
      "{\n",
      "  \"text\": \"Regular Council Meeting Minutes - Tuesday, April 9, 2024    R2024-71  Support for Community Emergency Preparedness Fund (CEPF) Councillor De Andrade Councillor Croal Council supports the Sunshine Coast Regional District applying for,  receiving, and managing Community Emergency Preparedness Fund (CEPF)  Evacuation Route Planning grant funding on behalf of the Town of Gibsons.  CARRIED\",\n",
      "  \"categories\": [\n",
      "    \"other__geographically-oriented\",\n",
      "    \"other__emergency_management\",\n",
      "    \"housing\",\n",
      "    \"injury_prevention\",\n",
      "    \"environmental_exposures__extreme_weather\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"environmental_exposures__extreme_weather\",\n",
      "    \"injury_prevention\",\n",
      "    \"youth_children\",\n",
      "    \"other__emergency_management\",\n",
      "    \"mental_health\",\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"environmental_exposures__air_quality_\"\n",
      "  ],\n",
      "  \"id\": 10\n",
      "}\n",
      "{\n",
      "  \"text\": \"R2024-72  Allocation of Community Event Fund: Councillor Croal Councillor De Andrade the community event funding request in the amount of $5,000.00 for  2024 Oceanfest be ;      AND the Town of Gibsons reinstates the Canada Day celebrations as  they were prior to COVID.  CARRIED\",\n",
      "  \"categories\": [\n",
      "    \"housing\"\n",
      "  ],\n",
      "  \"categories_by_vd\": [\n",
      "    \"housing\",\n",
      "    \"environmental_exposures__extreme_weather\",\n",
      "    \"food\",\n",
      "    \"climate_change\",\n",
      "    \"children___youth\",\n",
      "    \"indigenous_relations\"\n",
      "  ],\n",
      "  \"id\": 11\n",
      "}\n",
      "{\n",
      "  \"text\": \"NEXT MEETING    The next Regular meeting of Council to be held on Tuesday, April 23, 2024 at  7:00pm.      ADJOURNMENT    R2024-73 Councillor Croal Councillor Thompson the meeting be adjourned at 7:53pm.  CARRIED Rebecca Anderson, Corporate Officer  Silas White, Mayor\",\n",
      "  \"categories\": [],\n",
      "  \"categories_by_vd\": [\n",
      "    \"injury_prevention__youth_self-harm\",\n",
      "    \"housing\",\n",
      "    \"healthy_built_environments__community_design/planning\",\n",
      "    \"youth_children\"\n",
      "  ],\n",
      "  \"id\": 12\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# NLP\n",
    "pdf_folder = 'data/batch/'\n",
    "metting_name = \"gib_mcp_rgc_min__2024-04-09__01\"\n",
    "pdf_file = metting_name + \".pdf\"\n",
    "pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "# get the info of the meeting\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "context = {}\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "context['pdf'] = text\n",
    "# get the metadata\n",
    "metadata = fitz.open(pdf_path).metadata\n",
    "context['metadata'] = metadata\n",
    "# check if there is a txt with the same name\n",
    "txt_path = os.path.join(pdf_folder, pdf_file.replace('.pdf', '.txt'))\n",
    "if os.path.exists(txt_path):\n",
    "    with open(txt_path, 'r') as file:\n",
    "        context['transcript'] = file.read()\n",
    "\n",
    "else:\n",
    "        context['transcript'] = None\n",
    "\n",
    "df = pd.read_excel(\"data/themekeywordmap.xlsx\")\n",
    "category_keywords = {}\n",
    "for theme, group in df.groupby('theme'):\n",
    "    category_keywords[theme] = group['phrase'].tolist()\n",
    "\n",
    "split_patterns = [\n",
    "    r\"MOVED by\",\n",
    "    r\"SECONDED by\",\n",
    "    r\"WHEREAS\",\n",
    "    r\"THEREFORE BE IT RESOLVED THAT\",\n",
    "    r\"CARRIED UNANIMOUSLY\",\n",
    "    r\"REJECTED\",\n",
    "    r\"THAT\",\n",
    "    r\"APPROVED\",\n",
    "    r\"ADOPTED\",\n",
    "    r\"RESOLVED\"\n",
    "]\n",
    "Proposal_indicators = [\"Moved\", \"Seconded\", \"Motion\", \"Carried\", \"Proposal\", \"Passed\", \"Adopted\", \"Adoption\", \"Rejected\", \"Lost\", \"Moved\", \"approve\", \"Seconded\" , \"Adopt\", \"Resolution\", \"rejected\", \"Ordinance\", \"defeated\", \"discussed\", \"withdrawn\", \"tabled\", \"Amendment\", \"Amendment\", \"Recommendation\", \"granted\", \"Petition\", \"denied\", \"Vote\", \"result\"]\n",
    "# lower case it\n",
    "Proposal_indicators = [x.lower() for x in Proposal_indicators]\n",
    "segments = segment_document(context['pdf'], split_patterns)\n",
    "combined_segments = combine_segments_v3(segments, category_keywords, Proposal_indicators)\n",
    "len(combined_segments)\n",
    "\n",
    "# Load the FAISS index and category indices\n",
    "index = faiss.read_index('data/faiss_index.bin')\n",
    "category_indices = np.load('data/category_indices.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "# Verify categories and update segments\n",
    "for segment in combined_segments:\n",
    "    suggested_categories = query_vector_database(segment['text'], model, index, category_indices)\n",
    "    segment[\"categories_by_vd\"] = suggested_categories\n",
    "    segment[\"id\"] = combined_segments.index(segment)\n",
    "\n",
    "# Print the updated categorized segments\n",
    "for segment in combined_segments:\n",
    "    print(json.dumps(segment, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a utility function to get the dictionary from combined_segments with specific id\n",
    "# combined_segments = [{'id': <id1>, 'txt': <text 1>, 'category': <category 1>, \"category_by_vd\":< cat1>\"},\n",
    "#  {'id': <id2>, 'txt': <text 2>, 'category': <category 2>, \"category_by_vd\":< cat2>\"}, ...]\n",
    "\n",
    "def get_dict_by_id(combined_segments, id):\n",
    "    for segment in combined_segments:\n",
    "        if segment[\"id\"] == id:\n",
    "            return segment\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "### State\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        sprop: list of different suggested proposal dictionaries with key id, cat, prop by NER\n",
    "        last_id: last visited ID\n",
    "        count: number of unique proposal\n",
    "        suggested_proposal: suggested_proposal\n",
    "        output: dictionary of features and adjusted poposal\n",
    "        document: list of related document\n",
    "    \"\"\"\n",
    "    sprop: List[dict]\n",
    "    last_id: int\n",
    "    count: int\n",
    "    suggested_proposal: str\n",
    "    output: dict\n",
    "    document: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "final_dict = {0: {\"title\":\"\", \"full proposal\":\"\", \"theme\":[], \"vote_result\":\"\", \"future_date\":\"\"}}\n",
    "\n",
    "def iterate(state):\n",
    "    \"\"\"\n",
    "    Iterate over the sprop dictionary and return the last_id.\n",
    "    For the first time, call the iterate with input last_id as -1.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, sprop, that contains the next suggested proposal\n",
    "    \"\"\"\n",
    "    print(\"-------************ITERATE NODE***************-------\")\n",
    "    last_id = state[\"last_id\"]\n",
    "    last_id += 1\n",
    "    print(\"running for id: \", last_id)\n",
    "    \n",
    "    return {\"last_id\": last_id}\n",
    "\n",
    "def proposal_count(state):\n",
    "    \"\"\"\n",
    "    Determines whether the suggested_proposal is actually a proposal.\n",
    "    update the count state\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): change the count of the proposal\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---PROPOSAL COUNT NODE---\")\n",
    "    suggested_proposal = state[\"sprop\"][state[\"last_id\"]][\"text\"]\n",
    "    category = get_dict_by_id(state[\"sprop\"], state[\"last_id\"])[\"categories_by_vd\"]\n",
    "    \n",
    "    score = retrieval_counter.invoke({\"category\": category, \"suggested_proposal\": suggested_proposal})\n",
    "        \n",
    "    count = score[\"count\"]\n",
    "    return {\"count\": count,  \"suggested_proposal\": suggested_proposal}\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE NODE---\")\n",
    "    suggested_proposal = state[\"suggested_proposal\"]\n",
    "\n",
    "    # Retrieval\n",
    "    document = retriever.invoke(suggested_proposal)[:3]\n",
    "    document = [d.page_content for d in document]\n",
    "    return {\"document\": document}\n",
    "\n",
    "\n",
    "\n",
    "def get_proposal(state):\n",
    "    \"\"\"\n",
    "    Get all the suggested proposal from the suggested proposal.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, suggested_proposal, that contains the suggested proposal\n",
    "    \"\"\"\n",
    "    print(\"---GET PROPOSAL NODE---\")\n",
    "    all_proposals = state[\"suggested_proposal\"]\n",
    "    count = state[\"count\"]\n",
    "\n",
    "\n",
    "    prop_dict = retrieval_prop.invoke({\"all_proposals\": all_proposals, \"count\": count})\n",
    "    \n",
    "    state[\"sprop\"].extend(prop_dict.values())\n",
    "    return {\"sprop\": state[\"sprop\"]}\n",
    "\n",
    "\n",
    "\n",
    "def proposal_boundary(state):\n",
    "    \"\"\"\n",
    "    Determines the currect bounday of the suggested_proposal.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): change the count of the proposal\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ADJUST BOUNDARY NODE---\")\n",
    "    suggested_proposal = state[\"suggested_proposal\"]\n",
    "    document = state[\"document\"]\n",
    "    boundry = retrieval_boundary.invoke({\"document\": document, \"suggested_proposal\": suggested_proposal})\n",
    "    proposal_adj = adjusted_proposal_boundary(document, suggested_proposal, boundry)\n",
    "    state[\"output\"] = {\"full proposal\" :proposal_adj}\n",
    "    return { \"suggested_proposal\": suggested_proposal, \"document\": document, \"output\": state[\"output\"]}\n",
    "\n",
    "\n",
    "def proposal_features(state):\n",
    "    \"\"\"\n",
    "    Extract the features of the proposal.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): change the count of the proposal\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---EXTRACT FEATURES NODE---\")\n",
    "    suggested_proposal = state[\"suggested_proposal\"]\n",
    "    prop = get_dict_by_id(state[\"sprop\"], state[\"last_id\"])\n",
    "    document = state[\"document\"][:1]\n",
    "    result = chain_feature.invoke({'document':document ,'suggested_proposal': suggested_proposal, 'category': prop[\"categories_by_vd\"]})\n",
    "    # convert ProposalFeatures to dict\n",
    "    state[\"output\"].update(result.dict()) \n",
    "\n",
    "    # save to the final dict\n",
    "    final_dict[state[\"last_id\"]] = {k:v for k,v in state[\"output\"].items() if k != \"category_llm\"}\n",
    "    prop[\"categories_by_vd\"].extend(prop[\"categories\"])\n",
    "    # check if state[\"output\"][\"category_llm\"] is a list\n",
    "    if type(state[\"output\"][\"category_llm\"]) == list:\n",
    "        prop[\"categories_by_vd\"].extend(state[\"output\"][\"category_llm\"])\n",
    "    else:\n",
    "        prop[\"categories_by_vd\"].append(state[\"output\"][\"category_llm\"])\n",
    "\n",
    "    final_dict[state[\"last_id\"]]['theme'] = list(set(prop[\"categories_by_vd\"]))\n",
    "\n",
    "    return { \"suggested_proposal\": suggested_proposal, \"document\": document, \"output\": state[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conditional edge\n",
    "def route_count(state):\n",
    "    \"\"\"\n",
    "    Route based on the value of count.\n",
    "    0: Invalid proposal, go to iterate node\n",
    "    1: Valid proposal, go to retrieve node\n",
    "    >1: More than one proposal, go to get_suggested_proposal node\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTING based on validity---\")\n",
    "    grade = state[\"count\"]\n",
    "    if grade == 0:\n",
    "        print(\"Suggested proposal is NOT valid. Moving to the next one ... \")\n",
    "        return \"iterate\"\n",
    "    elif grade > 1:\n",
    "        print(\"Suggested proposal is MORE than one. Calling get_proposal ...\")\n",
    "        return \"get_proposal\"\n",
    "    else:\n",
    "        print(\"Suggested proposal is valid. retrieve the document ...\")\n",
    "        return \"retrieve\"\n",
    "    \n",
    "\n",
    "def stop_iteration(state):\n",
    "    \"\"\"\n",
    "    Stop the iteration if all the suggested proposal is visited.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK STOP ITERATION---\")\n",
    "    last_id = state[\"last_id\"]\n",
    "    if last_id == len(state[\"sprop\"]):\n",
    "        print(\"STOPPED!\")\n",
    "        return \"END\"\n",
    "    print(\"Continue ...\")\n",
    "    return \"proposal_count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"iterate\", iterate)  # iterate loop\n",
    "workflow.add_node(\"proposal_count\", proposal_count)  # count the proposal\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve related documents\n",
    "workflow.add_node(\"get_proposal\", get_proposal)  # get new proposal\n",
    "workflow.add_node(\"proposal_boundary\", proposal_boundary)  # adjust proposal boundary\n",
    "workflow.add_node(\"proposal_features\", proposal_features)  # extract proposal features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph build\n",
    "\n",
    "workflow.set_entry_point(\"iterate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"iterate\",\n",
    "    stop_iteration,\n",
    "    {\n",
    "        \"proposal_count\": \"proposal_count\",\n",
    "        \"END\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"proposal_count\",\n",
    "    route_count,\n",
    "    {\n",
    "        \"iterate\": \"iterate\",\n",
    "        \"get_proposal\": \"get_proposal\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"get_proposal\", \"iterate\")\n",
    "workflow.add_edge(\"retrieve\", \"proposal_boundary\")\n",
    "workflow.add_edge(\"proposal_boundary\", \"proposal_features\")\n",
    "workflow.add_edge(\"proposal_features\", \"iterate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------************ITERATE NODE***************-------\n",
      "running for id:  0\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  1\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  2\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  3\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  4\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  5\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  6\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  7\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  8\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  9\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  10\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  11\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is valid. retrieve the document ...\n",
      "'Finished running: proposal_count'\n",
      "---RETRIEVE NODE---\n",
      "'Finished running: retrieve'\n",
      "---ADJUST BOUNDARY NODE---\n",
      "'Finished running: proposal_boundary'\n",
      "---EXTRACT FEATURES NODE---\n",
      "'Finished running: proposal_features'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  12\n",
      "---CHECK STOP ITERATION---\n",
      "Continue ...\n",
      "'Finished running: iterate'\n",
      "---PROPOSAL COUNT NODE---\n",
      "---ROUTING based on validity---\n",
      "Suggested proposal is NOT valid. Moving to the next one ... \n",
      "'Finished running: proposal_count'\n",
      "-------************ITERATE NODE***************-------\n",
      "running for id:  13\n",
      "---CHECK STOP ITERATION---\n",
      "STOPPED!\n",
      "'Finished running: iterate'\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "inputs = {\"sprop\": combined_segments, \"last_id\": -1} #-1\n",
    "for output in app.stream(inputs,  {\"recursion_limit\": 1000}):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert final_dict to csv file with keys as row and each key of the value as column\n",
    "# it should look like this\n",
    "# meeting name, id ,title, full proposal, theme, vote_result, future_date\n",
    "import pandas as pd\n",
    "# first load the csv output if exists.\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(final_dict, orient='index')\n",
    "df[\"meeting_name\"] = metting_name   \n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={\"index\": \"id\"})\n",
    "df = df[[\"meeting_name\", \"id\", \"title\", \"full proposal\", \"theme\", \"vote_result\", \"future_date\"]]\n",
    "# replace None values with \"\"\n",
    "df = df.replace({None: \"NONE\"})\n",
    "\n",
    "# append df0 to end of df if meeting_name is not in df0, otherwize replace the row with the same meeting_name and append the rest\n",
    "\n",
    "try:\n",
    "    df0 = pd.read_csv(\"data/output\")\n",
    "    if metting_name in df0[\"meeting_name\"].unique():\n",
    "        # replace the row\n",
    "        df0 = df0[df0[\"meeting_name\"] != metting_name]\n",
    "        df = pd.concat([df, df0], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.concat([df, df0], ignore_index=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "df.to_csv(\"data/output\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
