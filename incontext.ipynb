{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path):\n",
    "#     \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     text = \"\"\n",
    "#     for page_num in range(len(doc)):\n",
    "#         page = doc.load_page(page_num)\n",
    "#         text += page.get_text()\n",
    "#     return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file with pypdf2.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "# Example usage:\n",
    "pdf_folder = 'data/'\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "\n",
    "# create a dictionary to store the text of each pdf and the metadata and if there is a txt with the same name\n",
    "contex = {}\n",
    "for pdf_file in pdf_files:\n",
    "    contex[pdf_file]={}\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    contex[pdf_file]['pdf'] = text\n",
    "    # get the metadata\n",
    "    #metadata = fitz.open(pdf_path).metadata\n",
    "    metadata = PdfReader(pdf_path).metadata\n",
    "    contex[pdf_file]['metadata'] = metadata\n",
    "    # check if there is a txt with the same name\n",
    "    txt_path = os.path.join(pdf_folder, pdf_file.replace('.pdf', '.txt'))\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r') as file:\n",
    "            contex[pdf_file]['transcript'] = file.read()\n",
    "\n",
    "    else:\n",
    "        contex[pdf_file]['transcript'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'gibsons',\n",
       " 'location type': 'municipality',\n",
       " 'meeting type': 'regular_council',\n",
       " 'data type': 'minutes',\n",
       " 'meeting date': Timestamp('2024-04-09 00:00:00'),\n",
       " 'transcript': 'Yes',\n",
       " 'comment': nan}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one file:\n",
    "pdf_folder = 'data/batch/'\n",
    "metting_name = \"gib_mcp_rgc_min__2024-04-09__01\"\n",
    "# read the info from a xlsx file\n",
    "import pandas as pd\n",
    "df = pd.read_excel(\"data/meetingmap.xlsx\")\n",
    "df = df.set_index('standard name')\n",
    "# get the info of the meeting\n",
    "meeting_info = df.loc[metting_name+\".pdf\"]\n",
    "# return all the columns for the meeting\n",
    "meeting_info = meeting_info.to_dict()\n",
    "meeting_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "pdf_files = [metting_name+\".pdf\"]\n",
    "\n",
    "# create a dictionary to store the text of each pdf and the metadata and if there is a txt with the same name\n",
    "contex = {}\n",
    "for pdf_file in pdf_files:\n",
    "    contex[pdf_file]={}\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    contex[pdf_file]['pdf'] = text\n",
    "    # get the metadata\n",
    "    metadata = fitz.open(pdf_path).metadata\n",
    "    contex[pdf_file]['metadata'] = metadata\n",
    "    # check if there is a txt with the same name\n",
    "    txt_path = os.path.join(pdf_folder, pdf_file.replace('.pdf', '.txt'))\n",
    "    if os.path.exists(txt_path):\n",
    "        with open(txt_path, 'r') as file:\n",
    "            contex[pdf_file]['transcript'] = file.read()\n",
    "\n",
    "    else:\n",
    "        contex[pdf_file]['transcript'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pdf', 'metadata', 'transcript'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contex[pdf_files[0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=512, overlap=256):\n",
    "    \"\"\"Divides text into overlapping chunks.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        chunk.append(sentence)\n",
    "        current_length += len(sentence.split())\n",
    "\n",
    "        if current_length >= chunk_size:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = chunk[-(overlap // len(sentence.split())):]  # Start next chunk with the overlap\n",
    "            current_length = len(\" \".join(chunk).split())\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage:\n",
    "input_text = [contex[pdf_file]['pdf'] for pdf_file in pdf_files]\n",
    "for text in input_text:\n",
    "    chunks = chunk_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better option\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1024, chunk_overlap=50\n",
    ")\n",
    "input_text = [contex[pdf_file]['pdf'] for pdf_file in pdf_files]\n",
    "for text in input_text:\n",
    "    all_splits = text_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "# gpt4all_kwargs = {'allow_download': 'True'}\n",
    "\n",
    "# embedding = GPT4AllEmbeddings(  model_name=model_name,\n",
    "#     gpt4all_kwargs=gpt4all_kwargs)\n",
    "# # Index\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=all_splits,\n",
    "#     collection_name=\"rag-chroma\",\n",
    "#     embedding=embedding,\n",
    "# )\n",
    "# retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi\n",
      "Processing chunk 1 of 2 ...\n",
      "3  new proposals was found in this chunk\n",
      "Warning, proposals in chunk 1 does not match the count\n",
      "Processing chunk 2 of 2 ...\n",
      "3  new proposals was found in this chunk\n",
      "Warning, proposals in chunk 2 does not match the count\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"llama3\"\n",
    "model_name = \"phi3\"\n",
    "\n",
    "\n",
    "model = Ollama(model=model_name)\n",
    "\n",
    "PROMPT_llama =\"\"\"<|system|>\n",
    "You are an assitance that find distinct proposals titles in a meeting note. You are provided with a context delimited by ### as a part of a meeting note. The goal is to find unique individual proposals. \n",
    "DO NOT use `\\ n` to indicate a new proposals. Only seperate them by ||. Only write the number of proposal once after your thinking step followed by ||. \n",
    "Your task is to find the count of unique proposal and write the title of the proposal(s) delimited by ||. If a proposal is not complete (does not have the result), do not count it. DO NOT write less than 5 or more than 15 words for each proposal.  \n",
    "\n",
    "Your output style should be this: \n",
    "<thinking steps> || <num proposal, n: int > || <proposal 1 title: str> || <proposal 2 title: str> ... || <proposal n title: str>\n",
    "\n",
    "Let's think step by step. Here are the steps to solve the task:\n",
    "1. Find the pattern in the chunk that indicates a proposal.\n",
    "2. Find the number of unique proposals in the chunk. \n",
    "3. Make sure all content about each proposal including the title, proposer, seconder, and the result are included and they are unique.\n",
    "4. Write a title (min 5 to max 15 words) for those proposals.\n",
    "\n",
    "Here is an example: \n",
    "Context: ### Minutes of the Council meeting of February 6, 2024, be approved.\\n CARRIED UNANIMOUSLY \\n Council Meeting\\n Minutes, February 27, 2024 3\\n 3. Council (City Finance and Services) \\n MOVED by Councillor Dominato\\n SECONDED by Councillor Carr\\n THAT the Minutes of the Council meeting following the Standing Committee on City\\n Finance and Services meeting of February 7, 2024, be approved.\\n CARRIED UNANIMOUSLY\\n 4. Court of Revision (Business Improvement Areas) - February 8, 2024\\n MOVED by Councillor Bligh\\n SECONDED by Councillor Zhou\\n  THAT the Minutes of the Court of Revision (Business Improvement Areas) meeting of\\n February 8, 2024, be approved.\\n CARRIED UNANIMOUSLY\\n MATTERS ADOPTED ON CONSENT\\n MOVED by Councillor Carr \\n Tax Rates Bylaw”:\\n THAT “Tax Rates Bylaw, 2024, No. 9017” be considered.\\n R2024-04-22/10\\n BYLAW FIRST,   ###\n",
    "\n",
    "Thinking steps:\n",
    "1. It is constantly using the word THAT to indicate a proposal. \n",
    "2. Found 4 proposals which has THAT \n",
    "3. The last proposal does not have the result, so it is not a complete proposal. \n",
    "4. I have 3 proposals, and will write short title for them. \n",
    "\n",
    "assitante response:\n",
    "|| 3 ||  the Minutes of the Council meeting of February 6, 2024, be approved. || the Minutes of the Council meeting following the Standing Committee on City Finance and Services, be approved || the Minutes of the Court of Revision (Business Improvement Areas) meeting be approved.\n",
    "<|end|>\n",
    "\n",
    "<|user|>\n",
    "Context:\n",
    "###\n",
    "{context}\n",
    "###\n",
    "<|end|>\n",
    "\n",
    "<|assistant|> \"\"\" \n",
    "\n",
    "PROMPT_phi =\"\"\"<|system|>\n",
    "You are an assitance that find all proposals mentioned in a meeting note. You are provided with a context delimited by ### as a part of a meeting note.\n",
    "Your task is to find the exact count of proposal, and then write short title of each proposal(s) delimited by <|sep|>. \n",
    "\n",
    "Let's think step by step. Here are the steps to solve the task:\n",
    "1. Find the pattern in the chunk that indicates a proposal.\n",
    "2. Find the count of all the proposals in the chunk: n. \n",
    "3. Make sure all content about each proposal including the title, proposer, seconder, and the result (carried or rejected) are included in the context.\n",
    "4. Write a short title (min 5 to max 15 words) for n proposals.\n",
    "\n",
    "RULES:\n",
    "- Your answer must not include any speculation or inference. Do not assume or change content. Only provide information that is explicitly stated in the context.\n",
    "- The number of used delimiter <|sep|> MUST BE EQUAL to the number of proposals found.\n",
    "- Each proposal title MUST be between 5 to 15 words. If it was longer, summarize it.\n",
    "- Only write the number of proposal once in the begining. \n",
    "- NEVER use `\\n` in your response.\n",
    "- An answer is considered grounded if **all** information in **every** sentence in the answer is **explicitly** mentioned in the source context, **no** extra information is added and **no** inferred information is added.\n",
    "\n",
    "Sample of correct assistant output:\n",
    "3 <|sep|>  title 1 ... <|sep|>  title 2 .... <|sep|> title 3 ...\n",
    "\n",
    "<|end|>\n",
    "\n",
    "<|user|>\n",
    "Context:\n",
    "###\n",
    "{context}\n",
    "###\n",
    "<|end|>\n",
    "\n",
    "<|assistant|> \"\"\" \n",
    "if model_name == \"llama3\":\n",
    "    print(\"llama\")\n",
    "    PROMPT = PROMPT_llama\n",
    "    delimiter = \"||\"\n",
    "elif model_name == \"phi3\":\n",
    "    print(\"phi\")\n",
    "    PROMPT = PROMPT_phi\n",
    "    delimiter = \"<|sep|>\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT)\n",
    "\n",
    "chain = prompt | model\n",
    "proposal_dictionary = {}\n",
    "proposal_count_dictionary = {}\n",
    "total_chunks = len(all_splits)\n",
    "iterator = 1\n",
    "while iterator <= total_chunks:\n",
    "    context = all_splits[iterator-1]\n",
    "    print(f'Processing chunk {iterator} of {total_chunks} ...')\n",
    "    latest_proposal = chain.invoke({'context': context})\n",
    "    # parse the latest_proposal and seperate them if it has special token <sep>\n",
    "    latest_proposal = latest_proposal.split(delimiter) \n",
    "    if len(latest_proposal) == 0:\n",
    "        print(f\"There was an error in chunk {iterator}, no {delimiter} found, running again\")\n",
    "        continue\n",
    "    try:\n",
    "        s = 0\n",
    "        int(latest_proposal[0])\n",
    "    except:\n",
    "        try:\n",
    "            s = 1\n",
    "            int(latest_proposal[1])\n",
    "            \n",
    "        except:\n",
    "            print(f\"There was an error in chunk {iterator}, running again\")\n",
    "            print(latest_proposal[1])\n",
    "            continue\n",
    "    print(f\"{latest_proposal[s]} new proposals was found in this chunk\")\n",
    "    proposal_dictionary[iterator-1] = latest_proposal[s+1:]\n",
    "    # checking for their lenght \n",
    "    for i in reversed(range(len(proposal_dictionary[iterator-1]))):\n",
    "        if len(proposal_dictionary[iterator-1][i]) < 20:\n",
    "            # remove this proposal\n",
    "            print(f\" Removing proposal (lenght limit) '{proposal_dictionary[iterator-1][i]}'\")\n",
    "            proposal_dictionary[iterator-1].pop(i)\n",
    "    proposal_count_dictionary[iterator-1] = int(latest_proposal[s])\n",
    "    if len(proposal_dictionary[iterator-1]) != proposal_count_dictionary[iterator-1]:\n",
    "        print(f\"Warning, proposals in chunk {iterator} does not match the count\")\n",
    "    iterator += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 ',\n",
       " ' Parcel Tax Roll Review Panel Setup  \\n',\n",
       " ' Canada BC Housing Benefit MOU Execution  \\n',\n",
       " ' CEPF Funding Support  \\n',\n",
       " ' Community Event Fund Approval and Reinstatement']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# convert proposal_dictionary to all_proposals list\n",
    "all_proposals = []\n",
    "chunks =[]\n",
    "for  key, value in proposal_dictionary.items():\n",
    "    all_proposals.extend(value)\n",
    "    # populate chunk with the keys on all elements of the value\n",
    "    chunks.extend([key for _ in range(len(value)) ])\n",
    "print(len(all_proposals))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4, 1: 1}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the count for each proposal in the chunks\n",
    "# count the number of elements in the chunks\n",
    "proposal_count_returned = {i:chunks.count(i) for i in set(chunks)}\n",
    "# find the chunks that the two dictionaries are different\n",
    "diff = {k: proposal_count_returned[k] - proposal_count_dictionary[k] for k in proposal_count_returned if k in proposal_count_dictionary and proposal_count_returned[k] != proposal_count_dictionary[k]}\n",
    "\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the is differece, we need to look at the proposals that are different\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_similarity_score(proposals):\n",
    "    pairwise_similarity = {}\n",
    "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
    "    # select two adjacent proposals and calculate the cosine similarity\n",
    "    for i in range(len(proposals)-1):\n",
    "        tfidf = vect.fit_transform([proposals[i], proposals[i+1]])                                                                                                                                                                                                                       \n",
    "        pairwise_similarity[f'{i}-{i+1}']=(tfidf * tfidf.T).toarray()[0][1]\n",
    "    return pairwise_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the proposals in chunk 0\n",
      "{'0-1': 0.2169338170113969, '1-2': 0.12083223046959846, '2-3': 0.03367858515950911, '3-4': 0.03367858515950911, '4-5': 0.2243746584076321, '5-6': 0.06655924660794856}\n",
      "Checking the proposals in chunk 1\n",
      "{'0-1': 0.0, '1-2': 0.0, '2-3': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for k in diff.keys():\n",
    "    print(f\"Checking the proposals in chunk {k}\")\n",
    "    print(get_similarity_score(proposal_dictionary[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the proposal dictionary to a txt file\n",
    "with open('data/proposals_{}.txt'.format(metting_name), 'w') as file:\n",
    "    json.dump(proposal_dictionary, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the proposal dictionary from a txt file\n",
    "with open('data/proposals_{}.txt'.format(metting_name), 'r') as file:\n",
    "    proposal_dictionary = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to check if the proposals are unique\n",
    "# we can use the cosine similarity to check if the proposals are unique\n",
    "\n",
    "# get the cosine similarity between the proposals\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
    "to_remove = []\n",
    "# select two adjacent proposals and calculate the cosine similarity\n",
    "for i in range(len(all_proposals)-1):\n",
    "    tfidf = vect.fit_transform([all_proposals[i], all_proposals[i+1]])                                                                                                                                                                                                                       \n",
    "    pairwise_similarity = tfidf * tfidf.T \n",
    "    if pairwise_similarity.toarray()[0][1] > 0.5:\n",
    "        to_remove.append(i+1)\n",
    "        print(f\"Proposal {i} and {i+1} are similar with score {pairwise_similarity.toarray()[0][1]}\")\n",
    "        print(all_proposals[i])\n",
    "        print(all_proposals[i+1])  \n",
    "        print('-------------------')                                                                                                                                                                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the similar proposals\n",
    "unique_proposals = [all_proposals[i] for i in range(len(all_proposals)) if i not in to_remove]\n",
    "unique_chunks = [chunks[i] for i in range(len(chunks)) if i not in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to dictionary\n",
    "unique_proposal_dictionary = {}\n",
    "for i in range(len(unique_chunks)):\n",
    "    if unique_chunks[i] in unique_proposal_dictionary:\n",
    "        unique_proposal_dictionary[unique_chunks[i]].append(unique_proposals[i])\n",
    "    else:\n",
    "        unique_proposal_dictionary[unique_chunks[i]] = [unique_proposals[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [' Proposal to adopt Regular Business Agenda of April 9, 2024 R2024-63 ',\n",
       "  ' Proposal for adoption of minutes from March 19, 2024 R2024-64 ',\n",
       "  ' Amendment to Development Permit Delegation Authority Bylaw No.1054-04, 2024 R2024-65 ',\n",
       "  ' Integration of 5-year capital plan into the financial plan with modifications except for Dog Park and Pickleball projects; approval of Healthy Harbours Project budget funded by surplus R2024-66 ',\n",
       "  ' Annual tax rates increase by 8% (3% general operations, 5% policing costs) Bylaw No. R2024-67 ',\n",
       "  ' Approval of the Financial Plan and Annual Tax Rate Bylaw for 2024-2028 R2024-68 ',\n",
       "  ' Review Panel on Parcel Taxes R2024-69'],\n",
       " 1: [' Parcel Tax Roll Review Panel Setup  \\n',\n",
       "  ' Canada BC Housing Benefit MOU Execution  \\n',\n",
       "  ' CEPF Funding Support  \\n',\n",
       "  ' Community Event Fund Approval and Reinstatement']}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_proposal_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sectionid</th>\n",
       "      <th>subsectionid</th>\n",
       "      <th>policy_prop</th>\n",
       "      <th>theme</th>\n",
       "      <th>terms</th>\n",
       "      <th>policy_prop_future_date</th>\n",
       "      <th>future_action_date_type</th>\n",
       "      <th>vote_result</th>\n",
       "      <th>theme_as_reviewed</th>\n",
       "      <th>terms_to_remove</th>\n",
       "      <th>terms_to_add</th>\n",
       "      <th>comments</th>\n",
       "      <th>Wrong Cut</th>\n",
       "      <th>future_date_as_reviewed</th>\n",
       "      <th>sentence_suggesting_future_action</th>\n",
       "      <th>voting_result_as_reviewed</th>\n",
       "      <th>Validated?</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standard_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gib_mcp_rgc_min__2024-04-09__01.pdf</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>R2024-66 2024-2028 Preliminary General Service...</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>the Dog\\nPark and Pickleball projects</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CARRIED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gib_mcp_rgc_min__2024-04-09__01.pdf</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>R2024-69 Parcel Tax Roll Review Panel - Water/...</td>\n",
       "      <td>housing</td>\n",
       "      <td>community</td>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>exact</td>\n",
       "      <td>CARRIED</td>\n",
       "      <td>drop</td>\n",
       "      <td>community</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gib_mcp_rgc_min__2024-04-09__01.pdf</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>R2024-72 Allocation of Community Event Fund:\\n...</td>\n",
       "      <td>housing</td>\n",
       "      <td>community</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CARRIED</td>\n",
       "      <td>drop</td>\n",
       "      <td>community</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     sectionid  subsectionid  \\\n",
       "standard_name                                                  \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf        4.0           1.0   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf        5.0           1.0   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf        6.0           3.0   \n",
       "\n",
       "                                                                           policy_prop  \\\n",
       "standard_name                                                                            \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf  R2024-66 2024-2028 Preliminary General Service...   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf  R2024-69 Parcel Tax Roll Review Panel - Water/...   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf  R2024-72 Allocation of Community Event Fund:\\n...   \n",
       "\n",
       "                                            theme  \\\n",
       "standard_name                                       \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf  unclassified   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf       housing   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf       housing   \n",
       "\n",
       "                                                                     terms  \\\n",
       "standard_name                                                                \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf  the Dog\\nPark and Pickleball projects   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                              community   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                              community   \n",
       "\n",
       "                                    policy_prop_future_date  \\\n",
       "standard_name                                                 \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                     NaN   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf              2024-05-07   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                     NaN   \n",
       "\n",
       "                                    future_action_date_type vote_result  \\\n",
       "standard_name                                                             \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                     NaN     CARRIED   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                   exact     CARRIED   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                     NaN     CARRIED   \n",
       "\n",
       "                                    theme_as_reviewed terms_to_remove  \\\n",
       "standard_name                                                           \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf               NaN             NaN   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf              drop       community   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf              drop       community   \n",
       "\n",
       "                                    terms_to_add comments Wrong Cut  \\\n",
       "standard_name                                                         \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf          NaN      NaN       NaN   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf          NaN      NaN       NaN   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf          NaN      NaN       NaN   \n",
       "\n",
       "                                    future_date_as_reviewed  \\\n",
       "standard_name                                                 \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                     NaT   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                     NaT   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                     NaT   \n",
       "\n",
       "                                    sentence_suggesting_future_action  \\\n",
       "standard_name                                                           \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                               NaN   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                               NaN   \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                               NaN   \n",
       "\n",
       "                                    voting_result_as_reviewed Validated?  \n",
       "standard_name                                                             \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                       NaN        Yes  \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                       NaN        Yes  \n",
       "gib_mcp_rgc_min__2024-04-09__01.pdf                       NaN        Yes  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the information for this specific meeting from the proposals_for_review.xlsx file (training only) sheet review\n",
    "df = pd.read_excel(\"data/proposals_for_review.xlsx\", sheet_name=\"review\")\n",
    "df = df.set_index('standard_name')\n",
    "# get the list of proposals where the meeting name is the same as the current meeting\n",
    "meeting_proposals = df.loc[metting_name+\".pdf\"]\n",
    "\n",
    "meeting_proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/proposals_for_review.xlsx\", sheet_name=\"themes\")\n",
    "themes = df['theme'].tolist()\n",
    "themes = ['Uncategorized', 'food', 'housing', 'children', 'climate', 'health', 'environment', 'indigenous', 'women', 'diversity', 'poverty', 'psychoactive', 'mental']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to parse a response from llama3 output: { \"name\": \"R2024-63\" }\n\n  \n\n\n\n\n\n ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 143\u001b[0m\n\u001b[1;32m    140\u001b[0m proposal \u001b[38;5;241m=\u001b[39m unique_proposal_dictionary[iterator][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    141\u001b[0m themes_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(themes)\n\u001b[0;32m--> 143\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthemes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mthemes_str\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproposal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m result\n",
      "File \u001b[0;32m/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2504\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2502\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2503\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2504\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4573\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4568\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4569\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4570\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4572\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4576\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/local-scratch/localhome/pagand/projects/ragprop/.venv/lib/python3.11/site-packages/langchain_experimental/llms/ollama_functions.py:418\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m         response \u001b[38;5;241m=\u001b[39m parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    419\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse a response from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_generation_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m         )\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(\n\u001b[1;32m    423\u001b[0m         generations\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    424\u001b[0m             ChatGeneration(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         ]\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m called_tool_arguments \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_input\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parsed_chat_result\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    436\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to parse a response from llama3 output: { \"name\": \"R2024-63\" }\n\n  \n\n\n\n\n\n "
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from typing import Optional\n",
    "\n",
    "# model_name = \"phi3\"\n",
    "model_name = \"llama3\"\n",
    "\n",
    "\n",
    "# Schema for structured response\n",
    "class Proposal(BaseModel):\n",
    "    section: str = Field(description=\"The section and subsection of the proposal\" , required=True)\n",
    "    title: str = Field(description=\"A short title of the proposal\", required=True)\n",
    "    policy_prop: str = Field(description=\"Full proposal content\", required=True)\n",
    "    theme: str = Field(description=f\"categorical value from themes: {themes}\", required=True)\n",
    "    vote_result: str = Field(description=\"The result of the vote\", required=True)\n",
    "    future_date: Optional[str] = Field(description=\"The future date of the proposal\")\n",
    "\n",
    "\n",
    "\n",
    "# Prompt template\n",
    "PROMPT_llama =\"\"\" <|system|> Use tool_calls from OllamaFunctions of langchain_experimental.llms.ollama_functions.OllamaFunctions\n",
    " The output should be in json format which is structured with the following schema:\n",
    "\n",
    "    section: str = Field(description=\"The section and subsection of the proposal\" , required=True)\n",
    "    title: str = Field(description=\"A short title of the proposal\", required=True)\n",
    "    policy_prop: str = Field(description=\"Full proposal content\", required=True)\n",
    "    theme: str = Field(description=f\"categorical value from themes\", required=True)\n",
    "    vote_result: str = Field(description=\"The result of the vote\", required=True)\n",
    "    future_date: Optional[str] = Field(description=\"The future date of the proposal\")\n",
    "  \n",
    "\n",
    "Except the future_date, all other fields are required.\n",
    "    \n",
    "RULES: \n",
    "1. The given proposal  is delimted by *** and followed after the PROPOSAL word. \n",
    "2. The  context  is delimited by ### and followed after the CONTEXT word.\n",
    "3. The themes is delimited by @@@ and followed after the THEMES word.\n",
    "4. Call the tool_calls OllamaFunctions of langchain_experimental.llms.ollama_functions.OllamaFunctions and format json.\n",
    "\n",
    "Example of correct assistant JSON output:\n",
    "\n",
    "section: \"5.2\",\n",
    "title: \" Development 4 story building.\",\n",
    "policy_prop: \"THAT to development a 4story bulding for mixed use with 5 rooms.\",\n",
    "theme: \"Housing\",\n",
    "vote_result: \"CARRIED UNANIMOUSLY\",\n",
    "future_date: \"February 6, 2024\"\n",
    "\n",
    "<|end|>\n",
    "\n",
    "<|user|>\n",
    "THEMES:\n",
    "@@@\n",
    "{themes}\n",
    "@@@\n",
    "\n",
    "Context:\n",
    "###\n",
    "{context}\n",
    "###\n",
    "\n",
    "PROPOSAL:\n",
    "###\n",
    "{proposal}\n",
    "###\n",
    "\n",
    "Human: Given the context, retrive section, title, policy_prop, theme, vote_result and future_date of the given proposal.\n",
    "<|end|>\n",
    "\n",
    "<|assistant|> \"\"\"\n",
    "\n",
    "\n",
    "# Prompt template\n",
    "PROMPT_phi =\"\"\" <|system|> Call AIMessage with the tool_calls from OllamaFunctions of langchain_experimental.llms.ollama_functions.OllamaFunctions with the model phi3\n",
    " the output should be in json format. Make sure you have all the required information. The model is structured with the following schema:\n",
    "\n",
    "section: Optional[str] = Field(description=\"The section and subsection of the proposal\" )\n",
    "title: str = Field(description=\"A short title of the proposal\", required=True)\n",
    "policy_prop: str = Field(description=\"Full proposal content\", required=True)\n",
    "theme: str = Field(description=f\"a single string value from THEMES given by user\", required=True)\n",
    "vote_result: Optional[str] = Field(description=\"The result of the vote\")\n",
    "future_date: Optional[str] = Field(description=\"The future date of the proposal\")\n",
    "\n",
    "\n",
    "Example of correct assistant JSON output:\n",
    "\n",
    "section: \"5.2\",\n",
    "title: \" Development 4 story building.\",\n",
    "policy_prop: \"THAT to development a 4story bulding for mixed use with 5 rooms.\",\n",
    "theme: \"Housing\",\n",
    "vote_result: \"CARRIED UNANIMOUSLY\",\n",
    "future_date: \"February 6, 2024\"\n",
    "\n",
    "RULES:\n",
    "- your output MUST HAVE the exact json format as the schema. DO NOT add any other keys, information.\n",
    "- Your answer must not include any speculation or inference. Do not assume or change dates and times. Only provide information that is explicitly stated in the context.\n",
    "- The proposal is delimtied by ***, context delimited by ###, and THEMES is after @@@.\n",
    "- An answer is considered grounded if **all** information in **every** sentence in the answer is **explicitly** mentioned in the source context, **no** extra information is added and **no** inferred information is added.\n",
    "- theme field is a **string** value selected from one of the THEMES. Only return a string. \n",
    "<|end|>\n",
    "\n",
    "<|user|>\n",
    "THEMES:\n",
    "@@@\n",
    "{themes}\n",
    "@@@\n",
    "\n",
    "Context:\n",
    "###\n",
    "{context}\n",
    "###\n",
    "\n",
    "PROPOSAL:\n",
    "###\n",
    "{proposal}\n",
    "###\n",
    "\n",
    "Human: Given the context, retrive section, title, policy_prop, theme, vote_result and future_date of the given proposal. \n",
    "<|end|>\n",
    "\n",
    "<|assistant|> \"\"\"\n",
    "\n",
    "if model_name == \"llama3\":\n",
    "    print(\"llama\")\n",
    "    PROMPT = PROMPT_llama\n",
    "elif model_name == \"phi3\":\n",
    "    print(\"phi\")\n",
    "    PROMPT = PROMPT_phi\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT)\n",
    "\n",
    "# Chain\n",
    "llm = OllamaFunctions(model=model_name,format=\"json\", keep_alive=-1, temperature=0)\n",
    "structured_llm = llm.with_structured_output(Proposal)\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "iterator = 0\n",
    "context = all_splits[iterator]\n",
    "proposal = unique_proposal_dictionary[iterator][0]\n",
    "themes_str = ', '.join(themes)\n",
    "\n",
    "result = chain.invoke({'themes':themes_str ,'context': context, 'proposal': proposal})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Alex', height=5.0, hair_color='blonde')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "model = OllamaFunctions(\n",
    "    model=\"phi3\", \n",
    "    keep_alive=-1,\n",
    "    format=\"json\", temperature=0\n",
    "    )\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"  <|system|> Call AIMessage with the tool_calls from OllamaFunctions of langchain_experimental.llms.ollama_functions.OllamaFunctions with the model phi3\n",
    " the output should be in json format. The model is structured with the following schema:\n",
    "\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "  <|end|> \n",
    "    <|user|>\n",
    "Context:\n",
    "Alex is 5 feet tall. \n",
    "Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "Claudia is a brunette and Alex is blonde.\n",
    "\n",
    "Human: {question}\n",
    "<|end|>\n",
    "\n",
    "<|assistant|> \"\"\"\n",
    ")\n",
    "\n",
    "structured_llm = model.with_structured_output(Person)\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "alex = chain.invoke(\"Describe Alex\")\n",
    "alex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
